{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/course2024-nlp/blob/main/EDA_with_ELYZA_japanese_Llama_2_7b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwVDII8TUWK1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    set_seed,\n",
        "    BitsAndBytesConfig,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "from transformers.modeling_outputs import ModelOutput\n",
        "from peft import LoraConfig, PeftModel\n",
        "from trl import SFTTrainer\n",
        "\n",
        "set_seed(123)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYBWq5ihUWK2"
      },
      "outputs": [],
      "source": [
        "def accuracy(model, tokenizer, corpus, labels, batch_size=4):\n",
        "    model.eval()\n",
        "    num_correct_answers, num_answers = 0, 0\n",
        "    for i in tqdm(range(0, len(corpus), batch_size)):\n",
        "        texts = corpus[i:i+batch_size]\n",
        "        encodings = tokenizer(texts, padding=True, return_tensors=\"pt\")\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**encodings)\n",
        "        predicted = outputs.logits.argmax(-1)\n",
        "        category = torch.tensor(labels[i:i+batch_size])\n",
        "        num_correct_answers += (predicted == category).sum()\n",
        "        num_answers += len(texts)\n",
        "    model.train()\n",
        "    return (num_correct_answers / num_answers).item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIXtsaeOUWK3"
      },
      "outputs": [],
      "source": [
        "def embed(model, tokenizer, corpus, batch_size=4):\n",
        "    model.eval()\n",
        "    pooled_hidden_states = []\n",
        "    for i in tqdm(range(0, len(corpus), batch_size)):\n",
        "        texts = corpus[i:i+batch_size]\n",
        "        encodings = tokenizer(texts, padding=True, return_tensors=\"pt\")\n",
        "        with torch.no_grad():\n",
        "            outputs = model.model(**encodings)\n",
        "        last_hidden_state = outputs.last_hidden_state\n",
        "        pad_token_id = model.config.pad_token_id\n",
        "        input_ids = encodings.input_ids\n",
        "        sequence_lengths = torch.eq(input_ids, pad_token_id).int().argmax(-1)\n",
        "        sequence_lengths = (sequence_lengths - 1) % input_ids.shape[-1]\n",
        "        temp_batch_size = input_ids.shape[0]\n",
        "        pooled_hidden_state = last_hidden_state[\n",
        "            torch.arange(temp_batch_size, device=last_hidden_state.device),\n",
        "            sequence_lengths]\n",
        "        pooled_hidden_state = pooled_hidden_state.float().cpu().numpy()\n",
        "        pooled_hidden_states.append(pooled_hidden_state)\n",
        "    model.train()\n",
        "    return np.concatenate(pooled_hidden_states)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAT1snA9UWK3"
      },
      "outputs": [],
      "source": [
        "def print_trainable_parameters(model):\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} \"\n",
        "        f\"|| all params: {all_param} \"\n",
        "        f\"|| trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W94xLvTBUWK3"
      },
      "outputs": [],
      "source": [
        "def show_trainable_parameters(model, show_all=False):\n",
        "    for param_name, param in model.named_parameters():\n",
        "        if param.requires_grad or show_all:\n",
        "            print(param_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0LyObD-UWK3"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\n",
        "    \"shunk031/livedoor-news-corpus\",\n",
        "    train_ratio=0.8, val_ratio=0.1, test_ratio=0.1,\n",
        "    random_state=42,\n",
        "    shuffle=True,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "num_categories = len(set(dataset[\"train\"][\"category\"]))\n",
        "max_seq_length = 512\n",
        "\n",
        "category_names = ['movie-enter', 'it-life-hack', 'kaden-channel', 'topic-news', 'livedoor-homme', 'peachy', 'sports-watch', 'dokujo-tsushin', 'smax']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6aeTw1TMUWK4",
        "outputId": "7539e06f-2a49-4d68-a2ed-7e3f4a7a5256",
        "colab": {
          "referenced_widgets": [
            "1bbb406c0e674f97853492c4fe082932",
            "90cf705c36634328aff8e1d641d1154a",
            "1f4b498939cb4e529b92f7f411bd97b9"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1bbb406c0e674f97853492c4fe082932",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/5894 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "90cf705c36634328aff8e1d641d1154a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/737 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1f4b498939cb4e529b92f7f411bd97b9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/736 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "label_pos_tags = [\"NOUN\", \"VERB\", \"PROPN\"]\n",
        "\n",
        "nlp = spacy.load(\"ja_core_news_sm\")\n",
        "corpus = {}\n",
        "for key in dataset:\n",
        "    corpus[key] = []\n",
        "    for text in tqdm(dataset[key][\"title\"]):\n",
        "        corpus[key].append(\" \".join(\n",
        "            [token.lemma_\n",
        "             for token in nlp(text) if token.pos_ in label_pos_tags\n",
        "            ]\n",
        "        ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtDSv896UWK4"
      },
      "source": [
        "* https://huggingface.co/docs/transformers/main/en/main_classes/quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJvq6uYMUWK5",
        "outputId": "a0a46872-9755-4193-ef2c-464af26570f8",
        "colab": {
          "referenced_widgets": [
            "62603604d3e34a109795184f8d09bf68"
          ]
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/masada/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "62603604d3e34a109795184f8d09bf68",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at elyza/ELYZA-japanese-Llama-2-7b and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model_name = \"elyza/ELYZA-japanese-Llama-2-7b\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_storage=torch.bfloat16,\n",
        ")\n",
        "\n",
        "pretrained = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=num_categories,\n",
        "    quantization_config=bnb_config,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    low_cpu_mem_usage=True,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name, max_seq_length=max_seq_length,\n",
        ")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "pretrained.config.pad_token_id = pretrained.config.eos_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCP5ShZTUWK5",
        "outputId": "c955fd35-1886-44ae-b365-d12a6a4623e2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LlamaForSequenceClassification(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
              "    (layers): ModuleList(\n",
              "      (0-31): 32 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaSdpaAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm()\n",
              "        (post_attention_layernorm): LlamaRMSNorm()\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm()\n",
              "  )\n",
              "  (score): Linear(in_features=4096, out_features=9, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pretrained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOUHR_xAUWK6",
        "outputId": "3c3d534c-0332-40f8-abfe-f526a9fedc04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 131375104 || all params: 1750376448 || trainable%: 7.505534260936308\n"
          ]
        }
      ],
      "source": [
        "print_trainable_parameters(pretrained)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jkc_5jLhUWK6",
        "outputId": "a10629eb-f63f-4b68-ca5f-e9a0c6b523fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model.embed_tokens.weight\n",
            "model.layers.0.input_layernorm.weight\n",
            "model.layers.0.post_attention_layernorm.weight\n",
            "model.layers.1.input_layernorm.weight\n",
            "model.layers.1.post_attention_layernorm.weight\n",
            "model.layers.2.input_layernorm.weight\n",
            "model.layers.2.post_attention_layernorm.weight\n",
            "model.layers.3.input_layernorm.weight\n",
            "model.layers.3.post_attention_layernorm.weight\n",
            "model.layers.4.input_layernorm.weight\n",
            "model.layers.4.post_attention_layernorm.weight\n",
            "model.layers.5.input_layernorm.weight\n",
            "model.layers.5.post_attention_layernorm.weight\n",
            "model.layers.6.input_layernorm.weight\n",
            "model.layers.6.post_attention_layernorm.weight\n",
            "model.layers.7.input_layernorm.weight\n",
            "model.layers.7.post_attention_layernorm.weight\n",
            "model.layers.8.input_layernorm.weight\n",
            "model.layers.8.post_attention_layernorm.weight\n",
            "model.layers.9.input_layernorm.weight\n",
            "model.layers.9.post_attention_layernorm.weight\n",
            "model.layers.10.input_layernorm.weight\n",
            "model.layers.10.post_attention_layernorm.weight\n",
            "model.layers.11.input_layernorm.weight\n",
            "model.layers.11.post_attention_layernorm.weight\n",
            "model.layers.12.input_layernorm.weight\n",
            "model.layers.12.post_attention_layernorm.weight\n",
            "model.layers.13.input_layernorm.weight\n",
            "model.layers.13.post_attention_layernorm.weight\n",
            "model.layers.14.input_layernorm.weight\n",
            "model.layers.14.post_attention_layernorm.weight\n",
            "model.layers.15.input_layernorm.weight\n",
            "model.layers.15.post_attention_layernorm.weight\n",
            "model.layers.16.input_layernorm.weight\n",
            "model.layers.16.post_attention_layernorm.weight\n",
            "model.layers.17.input_layernorm.weight\n",
            "model.layers.17.post_attention_layernorm.weight\n",
            "model.layers.18.input_layernorm.weight\n",
            "model.layers.18.post_attention_layernorm.weight\n",
            "model.layers.19.input_layernorm.weight\n",
            "model.layers.19.post_attention_layernorm.weight\n",
            "model.layers.20.input_layernorm.weight\n",
            "model.layers.20.post_attention_layernorm.weight\n",
            "model.layers.21.input_layernorm.weight\n",
            "model.layers.21.post_attention_layernorm.weight\n",
            "model.layers.22.input_layernorm.weight\n",
            "model.layers.22.post_attention_layernorm.weight\n",
            "model.layers.23.input_layernorm.weight\n",
            "model.layers.23.post_attention_layernorm.weight\n",
            "model.layers.24.input_layernorm.weight\n",
            "model.layers.24.post_attention_layernorm.weight\n",
            "model.layers.25.input_layernorm.weight\n",
            "model.layers.25.post_attention_layernorm.weight\n",
            "model.layers.26.input_layernorm.weight\n",
            "model.layers.26.post_attention_layernorm.weight\n",
            "model.layers.27.input_layernorm.weight\n",
            "model.layers.27.post_attention_layernorm.weight\n",
            "model.layers.28.input_layernorm.weight\n",
            "model.layers.28.post_attention_layernorm.weight\n",
            "model.layers.29.input_layernorm.weight\n",
            "model.layers.29.post_attention_layernorm.weight\n",
            "model.layers.30.input_layernorm.weight\n",
            "model.layers.30.post_attention_layernorm.weight\n",
            "model.layers.31.input_layernorm.weight\n",
            "model.layers.31.post_attention_layernorm.weight\n",
            "model.norm.weight\n",
            "score.weight\n"
          ]
        }
      ],
      "source": [
        "show_trainable_parameters(pretrained)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arR5HcFfUWK6"
      },
      "outputs": [],
      "source": [
        "class MyNetForClassification(nn.Module):\n",
        "    def __init__(self, pretrained):\n",
        "        super().__init__()\n",
        "        self.pretrained = pretrained\n",
        "        self.config = self.pretrained.config\n",
        "\n",
        "    def forward(\n",
        "        self, input_ids, category=None, attention_mask=None,\n",
        "        output_attentions=None, output_hidden_states=None,\n",
        "        return_dict=None, inputs_embeds=None, labels=None,\n",
        "    ):\n",
        "        outputs = self.pretrained(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        loss_fct = nn.CrossEntropyLoss()\n",
        "        loss = loss_fct(outputs.logits, category)\n",
        "        return ModelOutput(\n",
        "            loss=loss,\n",
        "            logits=outputs.logits,\n",
        "            past_key_values=outputs.past_key_values,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "model = MyNetForClassification(pretrained)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALdkW79dUWK6",
        "outputId": "27d7f67d-7bad-4c15-93b1-a43df63d4c9a",
        "colab": {
          "referenced_widgets": [
            "2f21bd931012433bbb3c318e5d77279d",
            "d34a59c873734952965f976903b579b4",
            "539049fc27d247a0b7234a28d90589c3"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2f21bd931012433bbb3c318e5d77279d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1474 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d34a59c873734952965f976903b579b4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/185 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "539049fc27d247a0b7234a28d90589c3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/184 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "embeddings = {}\n",
        "for key in dataset:\n",
        "    embeddings[key] = embed(model.pretrained, tokenizer, dataset[key][\"title\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omZsbIRVUWK6"
      },
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer(min_df=10, max_df=0.1, lowercase=False)\n",
        "vectorizer.fit(corpus[\"train\"])\n",
        "vocab = np.array(vectorizer.get_feature_names_out())\n",
        "X_train = vectorizer.transform(corpus[\"train\"]).toarray()\n",
        "vocab_embeddings = np.dot((X_train / X_train.sum(0)).T, embeddings[\"train\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKox_Yh2UWK6"
      },
      "outputs": [],
      "source": [
        "n_clusters = 30\n",
        "kmeans = KMeans(n_clusters=n_clusters, n_init='auto', random_state=123)\n",
        "kmeans.fit(embeddings[\"train\"])\n",
        "centers = kmeans.cluster_centers_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pX1Pt1D_UWK6",
        "outputId": "46335aaf-c96b-4864-9485-3e7f7d2b79fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[5, 54, 57, 63, 77, 88, 90, 91, 96, 103, 104, 122, 157, 162, 166, 168, 169, 172, 178, 204, 268, 269, 272, 279, 316, 358, 373, 378, 421, 634]\n"
          ]
        }
      ],
      "source": [
        "unique, counts = np.unique(kmeans.labels_, return_counts=True)\n",
        "size_dict = dict(zip(unique, counts))\n",
        "print(sorted([item[1] for item in size_dict.items()]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZ0nCGFXUWK7",
        "outputId": "37eaff01-d5d3-4032-fe84-31665f5e1696"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 過ごす ひとり 時間 マザー 掲示 ストア 込む GHz 感動 ホテル よる 対する 連続 HD 贈る tab メイク デュアルコア シリーズ タイム\n",
            "1 候補 トレンド 婚活 ブーム 姉妹 過ごす ブランド ボール 自動車 男子 コレ ケーキ デート あり 悩み 違い 結婚 とも 真央 ドキドキ\n",
            "2 殺到 批判 非難 発言 激怒 続出 ブログ 謝罪 炎上 怒り ファン 言及 ツイート コメント 暴露 連発 メンバー 逮捕 キム ネット\n",
            "3 アプリ android iPhone ねこ 操作 ipad デザイン ゲーム 掴める チャンス まとめる 使う 機能 for 使える 管理 チェック 動画 わかる レビュー\n",
            "4 オトナ 恋人 インタビュー 人生 映画 家族 作品 編集 ヒロイン マンガ 違う まとめ コミック いく 読み 絶対 見る 分かる 週末 少女\n",
            "5 得る 虎の巻 ワザ ファイル 知る PC 管理 操作 役立つ 活用 ソフト まとめる Word 使う テクニック 便利 IT クラウド デジ 表示\n",
            "6 トレンド 過ごす ブランド 自動車 ドキドキ ブーム 婚活 コレ デート 候補 悩み 注意 ショー ケーキ 効果 あり ボール 愛す アイテム 男性\n",
            "7 デジ ipad パソコン チェック 売れ筋 機能 デザイン 操作 電子 レポート 音声 動画 わかる カメラ 家電 アプリ 試す iPhone マーク 使える\n",
            "8 解禁 予告 公開 映像 ポスター 主演 ハリウッド 豪華 ストーリー ナイト ダーク アベンジャーズ 決定 主題 来日 上陸 記録 ディズニー 突破 リアル\n",
            "9 掲示 対する 橋下 発言 物議 中島 ネット 賛否 韓流 両論 番組 怒り 行為 社長 事故 非難 言葉 出演 akb 大阪\n",
            "10 高級 少女 奇跡 かける アイドル 天才 キャラ 批評 込む 女優 美人 成功 隠す 飲む エンター ヒロイン いう 続く たち 編集\n",
            "11 理由 事情 付く たち 人間 こと 本音 学ぶ ため 見る 秘密 合う しまう 独女 秘訣 聞く 時間 女子 すぎる 読む\n",
            "12 部屋 説教 辛口 年収 図鑑 Vol プレイヤー 研究 ビジネスマン 活動 人事 ウラ 転職 会社 ソーシャル 教える まま メディア ビジネス 採用\n",
            "13 ディズニー ベルセルク アベンジャーズ 卒業 誕生 上陸 スポーツ 対決 デビュー 驚愕 DVD アカデミー 突破 主演 ナイト 恐怖 バトル コラボ ハリウッド ダーク\n",
            "14 国内 イー アクセス Android 発表 プラチナ lte wimax 追加 機種 向け タブレット コンテンツ 端末 複数 Fi Wi 対応 バンド HD\n",
            "15 予定 Android 予約 NTT インチ 開始 ソフトバンク ドコモ 機種 バージョン 事前 発表 向け xi タブレット ICS Mobile 対応 Play phone\n",
            "16 SNS 話題 わかる 家電 くれる 視聴 見える 原因 インターネット チェック テレビ 広がる 撮る 感覚 写真 カメラ 撮影 ゲーム 売れ筋 未来\n",
            "17 なれる スイーツ 癒し ムービー パワー 美人 ファッション アナタ シリーズ 東京 美容 オリジナル ピンク カフェ 贈る ため ホテル 編集 メイク 合う\n",
            "18 星野 ノム watch SPORTS 引退 監督 優勝 代表 告白 圭佑 巨人 松井 本田 意味 苦言 吉田 なでしこ 落合 柔道 田中\n",
            "19 アニメ 成功 もの 隠す 時間 すぎる くる 見る 2人 美女 目指す 来る 思う 人気 する たち 超える さん ある 人類\n",
            "20 特集 感覚 ビデオ オススメ 撮る 生活 salon チャンス ワン 連動 カメラ 家電 見える わかる チェック ゴルフ 講座 まとめる 撮影 連載\n",
            "21 明かす 次ぐ 語る 苦言 田中 交際 報道 絶賛 告白 ファン 日本 衝撃 受ける 吉田 石井 アナ 監督 引退 出演 復帰\n",
            "22 週間 ランキング ライフスタイル 登録 ビューティー お気に入り 記事 ビューティ 行動 恋愛 みんな 彼氏 レシピ ダイエット UP 方法 作る 解消 OK 美肌\n",
            "23 騒動 掲示 炎上 前田 対する 敦子 卒業 投手 ネット メンバー NHK 高橋 番組 自殺 状態 超える akb 名前 感動 ツイッター\n",
            "24 presented cafe ゆるい by あらわれる よめ くさる はじめる 風呂 自転車 Vol 教える オンナ ひとり 東京 リラックス 変える ガールズ まま 会える\n",
            "25 レポート 試す optimus マーク 画面 操作 機能 ipad ディスプレイ note sim galaxy ルーター 動画 HTC デザイン 専用 充電 音声 デジ\n",
            "26 終了 試写 招待 セット プレゼント プレミア 記念 限定 レビューアー 美肌 募集 豪華 スペシャル オープン ホテル 商品 期間 突破 オンライン コラボ\n",
            "27 提供 更新 ソフトウェア 具合 開始 KDDI IS NTT arrows phone isw 起動 Mobile ソフトバンク SH AQUOS ドコモ SII ブラウザ 向け\n",
            "28 限定 開催 豪華 オープン 先行 公開 コラボ 期間 商品 ディズニー 記念 オンライン In キャンペーン 上陸 募集 決定 周年 突破 リアル\n",
            "29 疑問 韓流 中島 対する 驚き 怒り 橋下 非難 フジテレビ やめる 高橋 大阪 移籍 発言 ユーザー 批判 掲示 NHK ネット 報道\n"
          ]
        }
      ],
      "source": [
        "topic_words = []\n",
        "distances = ((centers[np.newaxis,:,:] - vocab_embeddings[:,np.newaxis,:]) ** 2).sum(-1)\n",
        "for i in range(distances.shape[-1]):\n",
        "    indices = np.argsort(distances[:,i])\n",
        "    topic_words.append(f\"{i:d} \" + \" \".join(list(vocab[indices[:20]])))\n",
        "print(\"\\n\".join(topic_words))\n",
        "#with open(\"topic_words.txt\", \"w\") as f:\n",
        "#    f.write(\"\\n\".join(topic_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JV0MZv6jUWK7"
      },
      "outputs": [],
      "source": [
        "peft_config = LoraConfig(\n",
        "    r=32,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEbkcNL8UWK7"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=8,\n",
        "    output_dir=\"outputs_cls\",\n",
        "    label_names=[\"category\"],\n",
        "    max_steps=500,\n",
        "    eval_steps=100,\n",
        "    logging_steps=100,\n",
        "    save_steps=100,\n",
        "    learning_rate=5e-5,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    logging_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    load_best_model_at_end=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKgVTwTgUWK7",
        "outputId": "078e78d3-f7f0-4cba-b793-798083813c2c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        }
      ],
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    tokenizer=tokenizer,\n",
        "    max_seq_length=max_seq_length,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"validation\"],\n",
        "    dataset_text_field=\"title\",\n",
        "    peft_config=peft_config,\n",
        ")\n",
        "trainer.train_dataset = trainer.train_dataset.add_column(\n",
        "    \"category\", dataset[\"train\"][\"category\"],\n",
        ")\n",
        "trainer.eval_dataset = trainer.eval_dataset.add_column(\n",
        "    \"category\", dataset[\"validation\"][\"category\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BK5Nc0qKUWK7",
        "outputId": "3ff12c8a-c8de-4505-b25b-4dad6855e60c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 79953920 || all params: 1830330368 || trainable%: 4.368278065962789\n"
          ]
        }
      ],
      "source": [
        "print_trainable_parameters(trainer.model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8praJKHUWK7",
        "outputId": "d4fa1243-5178-42ce-b4b1-e70b39d05094"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "base_model.model.pretrained.model.layers.0.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.0.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.0.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.0.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.0.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.0.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.0.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.0.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.0.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.0.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.0.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.0.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.0.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.0.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.1.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.1.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.1.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.1.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.1.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.1.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.1.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.1.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.1.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.1.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.1.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.1.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.1.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.1.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.2.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.2.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.2.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.2.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.2.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.2.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.2.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.2.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.2.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.2.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.2.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.2.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.2.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.2.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.3.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.3.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.3.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.3.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.3.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.3.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.3.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.3.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.3.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.3.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.3.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.3.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.3.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.3.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.4.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.4.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.4.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.4.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.4.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.4.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.4.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.4.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.4.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.4.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.4.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.4.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.4.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.4.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.5.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.5.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.5.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.5.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.5.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.5.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.5.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.5.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.5.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.5.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.5.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.5.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.5.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.5.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.6.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.6.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.6.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.6.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.6.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.6.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.6.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.6.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.6.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.6.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.6.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.6.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.6.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.6.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.7.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.7.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.7.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.7.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.7.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.7.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.7.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.7.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.7.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.7.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.7.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.7.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.7.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.7.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.8.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.8.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.8.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.8.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.8.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.8.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.8.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.8.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.8.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.8.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.8.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.8.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.8.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.8.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.9.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.9.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.9.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.9.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.9.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.9.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.9.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.9.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.9.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.9.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.9.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.9.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.9.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.9.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.10.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.10.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.10.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.10.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.10.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.10.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.10.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.10.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.10.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.10.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.10.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.10.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.10.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.10.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.11.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.11.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.11.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.11.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.11.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.11.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.11.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.11.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.11.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.11.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.11.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.11.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.11.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.11.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.12.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.12.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.12.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.12.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.12.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.12.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.12.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.12.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.12.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.12.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.12.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.12.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.12.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.12.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.13.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.13.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.13.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.13.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.13.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.13.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.13.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.13.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.13.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.13.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.13.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.13.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.13.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.13.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.14.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.14.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.14.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.14.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.14.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.14.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.14.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.14.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.14.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.14.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.14.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.14.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.14.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.14.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.15.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.15.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.15.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.15.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.15.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.15.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.15.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.15.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.15.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.15.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.15.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.15.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.15.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.15.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.16.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.16.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.16.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.16.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.16.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.16.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.16.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.16.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.16.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.16.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.16.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.16.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.16.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.16.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.17.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.17.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.17.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.17.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.17.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.17.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.17.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.17.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.17.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.17.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.17.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.17.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.17.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.17.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.18.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.18.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.18.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.18.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.18.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.18.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.18.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.18.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.18.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.18.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.18.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.18.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.18.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.18.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.19.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.19.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.19.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.19.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.19.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.19.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.19.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.19.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.19.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.19.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.19.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.19.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.19.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.19.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.20.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.20.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.20.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.20.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.20.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.20.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.20.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.20.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.20.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.20.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.20.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.20.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.20.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.20.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.21.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.21.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.21.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.21.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.21.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.21.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.21.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.21.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.21.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.21.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.21.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.21.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.21.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.21.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.22.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.22.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.22.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.22.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.22.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.22.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.22.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.22.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.22.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.22.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.22.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.22.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.22.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.22.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.23.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.23.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.23.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.23.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.23.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.23.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.23.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.23.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.23.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.23.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.23.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.23.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.23.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.23.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.24.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.24.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.24.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.24.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.24.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.24.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.24.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.24.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.24.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.24.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.24.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.24.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.24.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.24.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.25.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.25.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.25.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.25.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.25.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.25.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.25.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.25.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.25.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.25.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.25.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.25.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.25.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.25.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.26.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.26.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.26.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.26.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.26.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.26.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.26.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.26.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.26.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.26.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.26.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.26.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.26.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.26.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.27.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.27.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.27.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.27.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.27.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.27.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.27.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.27.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.27.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.27.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.27.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.27.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.27.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.27.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.28.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.28.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.28.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.28.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.28.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.28.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.28.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.28.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.28.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.28.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.28.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.28.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.28.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.28.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.29.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.29.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.29.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.29.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.29.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.29.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.29.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.29.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.29.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.29.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.29.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.29.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.29.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.29.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.30.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.30.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.30.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.30.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.30.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.30.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.30.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.30.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.30.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.30.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.30.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.30.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.30.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.30.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.31.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.31.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.31.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.31.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.31.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.31.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.31.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.31.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.31.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.31.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.31.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.31.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.31.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.31.mlp.down_proj.lora_B.default.weight\n"
          ]
        }
      ],
      "source": [
        "show_trainable_parameters(trainer.model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GB08j58hUWK7"
      },
      "outputs": [],
      "source": [
        "model.pretrained.score.weight.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3R1_8FoUWK7",
        "outputId": "f73558c1-9ba4-4750-981a-7fb6ba6e1ec8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "base_model.model.pretrained.model.layers.0.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.0.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.0.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.0.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.0.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.0.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.0.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.0.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.0.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.0.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.0.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.0.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.0.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.0.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.1.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.1.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.1.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.1.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.1.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.1.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.1.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.1.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.1.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.1.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.1.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.1.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.1.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.1.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.2.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.2.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.2.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.2.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.2.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.2.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.2.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.2.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.2.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.2.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.2.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.2.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.2.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.2.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.3.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.3.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.3.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.3.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.3.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.3.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.3.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.3.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.3.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.3.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.3.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.3.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.3.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.3.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.4.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.4.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.4.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.4.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.4.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.4.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.4.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.4.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.4.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.4.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.4.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.4.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.4.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.4.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.5.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.5.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.5.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.5.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.5.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.5.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.5.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.5.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.5.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.5.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.5.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.5.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.5.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.5.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.6.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.6.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.6.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.6.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.6.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.6.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.6.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.6.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.6.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.6.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.6.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.6.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.6.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.6.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.7.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.7.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.7.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.7.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.7.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.7.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.7.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.7.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.7.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.7.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.7.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.7.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.7.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.7.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.8.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.8.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.8.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.8.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.8.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.8.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.8.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.8.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.8.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.8.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.8.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.8.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.8.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.8.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.9.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.9.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.9.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.9.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.9.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.9.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.9.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.9.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.9.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.9.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.9.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.9.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.9.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.9.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.10.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.10.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.10.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.10.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.10.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.10.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.10.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.10.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.10.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.10.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.10.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.10.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.10.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.10.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.11.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.11.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.11.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.11.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.11.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.11.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.11.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.11.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.11.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.11.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.11.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.11.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.11.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.11.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.12.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.12.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.12.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.12.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.12.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.12.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.12.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.12.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.12.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.12.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.12.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.12.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.12.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.12.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.13.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.13.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.13.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.13.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.13.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.13.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.13.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.13.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.13.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.13.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.13.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.13.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.13.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.13.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.14.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.14.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.14.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.14.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.14.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.14.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.14.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.14.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.14.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.14.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.14.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.14.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.14.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.14.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.15.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.15.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.15.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.15.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.15.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.15.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.15.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.15.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.15.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.15.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.15.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.15.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.15.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.15.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.16.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.16.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.16.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.16.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.16.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.16.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.16.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.16.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.16.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.16.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.16.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.16.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.16.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.16.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.17.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.17.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.17.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.17.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.17.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.17.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.17.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.17.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.17.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.17.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.17.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.17.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.17.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.17.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.18.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.18.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.18.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.18.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.18.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.18.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.18.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.18.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.18.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.18.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.18.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.18.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.18.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.18.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.19.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.19.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.19.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.19.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.19.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.19.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.19.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.19.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.19.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.19.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.19.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.19.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.19.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.19.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.20.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.20.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.20.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.20.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.20.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.20.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.20.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.20.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.20.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.20.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.20.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.20.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.20.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.20.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.21.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.21.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.21.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.21.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.21.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.21.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.21.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.21.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.21.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.21.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.21.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.21.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.21.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.21.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.22.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.22.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.22.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.22.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.22.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.22.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.22.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.22.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.22.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.22.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.22.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.22.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.22.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.22.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.23.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.23.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.23.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.23.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.23.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.23.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.23.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.23.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.23.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.23.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.23.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.23.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.23.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.23.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.24.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.24.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.24.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.24.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.24.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.24.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.24.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.24.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.24.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.24.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.24.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.24.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.24.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.24.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.25.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.25.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.25.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.25.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.25.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.25.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.25.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.25.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.25.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.25.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.25.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.25.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.25.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.25.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.26.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.26.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.26.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.26.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.26.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.26.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.26.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.26.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.26.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.26.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.26.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.26.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.26.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.26.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.27.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.27.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.27.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.27.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.27.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.27.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.27.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.27.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.27.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.27.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.27.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.27.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.27.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.27.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.28.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.28.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.28.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.28.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.28.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.28.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.28.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.28.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.28.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.28.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.28.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.28.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.28.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.28.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.29.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.29.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.29.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.29.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.29.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.29.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.29.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.29.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.29.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.29.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.29.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.29.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.29.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.29.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.30.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.30.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.30.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.30.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.30.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.30.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.30.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.30.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.30.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.30.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.30.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.30.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.30.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.30.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.31.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.31.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.31.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.31.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.31.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.31.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.31.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.31.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.31.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.31.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.31.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.31.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.model.layers.31.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.pretrained.model.layers.31.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.pretrained.score.weight\n"
          ]
        }
      ],
      "source": [
        "show_trainable_parameters(trainer.model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_k9z3NhUWK7",
        "outputId": "0132c707-14cf-4f8c-8d97-459f1bcc5d2c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 08:38, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.025600</td>\n",
              "      <td>0.437011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.394600</td>\n",
              "      <td>0.301558</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.219000</td>\n",
              "      <td>0.318471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.177400</td>\n",
              "      <td>0.309491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.092500</td>\n",
              "      <td>0.297548</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=500, training_loss=0.3818086204528809, metrics={'train_runtime': 519.5608, 'train_samples_per_second': 30.795, 'train_steps_per_second': 0.962, 'total_flos': 0.0, 'train_loss': 0.3818086204528809, 'epoch': 2.7137042062415198})"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()\n",
        "#trainer.model.save_pretrained(\"models/lora/\" + model_name)\n",
        "#model = PeftModel.from_pretrained(model, \"models/lora/\" + model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjhXOxk8UWK8",
        "outputId": "c702439c-baad-412a-f0ee-2dddc015fb50",
        "colab": {
          "referenced_widgets": [
            "8879f6bae6c040cb8a6eeaf118bc87f0"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8879f6bae6c040cb8a6eeaf118bc87f0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/185 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0.9145182967185974"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "accuracy(trainer.model.pretrained, tokenizer, dataset[\"validation\"][\"title\"], dataset[\"validation\"][\"category\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hANAfySRUWK8",
        "outputId": "cb5b54b2-a2be-48ae-c790-e4644cd81621",
        "colab": {
          "referenced_widgets": [
            "fe136f732d5e4ec7b950e426c8ca8808",
            "95105e630f244e158d00a503a9ce8771",
            "b504412aec084b59851b86ef6be60105"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fe136f732d5e4ec7b950e426c8ca8808",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1474 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "95105e630f244e158d00a503a9ce8771",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/185 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b504412aec084b59851b86ef6be60105",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/184 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "embeddings = {}\n",
        "for key in dataset:\n",
        "    embeddings[key] = embed(trainer.model.pretrained, tokenizer, dataset[key][\"title\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJZLx-3eUWK8"
      },
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer(min_df=10, max_df=0.1, lowercase=False)\n",
        "vectorizer.fit(corpus[\"train\"])\n",
        "vocab = np.array(vectorizer.get_feature_names_out())\n",
        "X_train = vectorizer.transform(corpus[\"train\"]).toarray()\n",
        "vocab_embeddings = np.dot((X_train / X_train.sum(0)).T, embeddings[\"train\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYvLq_ATUWK8"
      },
      "outputs": [],
      "source": [
        "n_clusters = 30\n",
        "kmeans = KMeans(n_clusters=n_clusters, n_init='auto', random_state=123)\n",
        "kmeans.fit(embeddings[\"train\"])\n",
        "centers = kmeans.cluster_centers_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQMfHEsyUWK8",
        "outputId": "edc08907-0558-4ef5-8fde-92303fbc2d81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[33, 44, 58, 63, 78, 89, 101, 118, 119, 122, 122, 124, 134, 135, 138, 155, 161, 162, 234, 235, 261, 265, 307, 307, 333, 380, 393, 400, 409, 414]\n"
          ]
        }
      ],
      "source": [
        "unique, counts = np.unique(kmeans.labels_, return_counts=True)\n",
        "size_dict = dict(zip(unique, counts))\n",
        "print(sorted([item[1] for item in size_dict.items()]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-3lsIdyUWK8",
        "outputId": "84755aee-2eb9-45cb-cacb-dec5f70c8841"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 チャンス iPhone 容量 使う ipad デザイン 感覚 ゲーム 操作 バッテリー 機能 情報 管理 使える 動画 通信 満載 便利 撮る 裏技\n",
            "1 運命 女子 学ぶ 魅力 事情 変える 大人 食べる 贈る トーク 続ける 効く 上げる 飲む 恋人 かける こと 秘訣 人生 会える\n",
            "2 非難 掲示 殺到 批判 発言 物議 次ぐ 怒り 波紋 賛否 両論 続出 騒然 ネット 橋下 母親 対する 疑問 騒動 中島\n",
            "3 SPORTS watch 星野 斎藤 巨人 岡田 引退 試合 ダルビッシュ 田中 開幕 ノム 監督 長友 吉田 真央 本田 野球 楽天 松井\n",
            "4 話題 発生 アップル 原因 家電 今度 インターネット パソコン 視聴 大丈夫 SNS センター 売れる 電子 未来 テレビ ページ 広がる パナソニック わかる\n",
            "5 読み 週末 まとめ エンター 映画 DVD 編集 天才 批評 女優 家族 ヒロイン 作品 超える 泣く ぶり 経験 獲得 描く 国際\n",
            "6 開始 ソフトバンク ドコモ NTT 予定 イー wimax phone KDDI Android au lte tab 向け xi SH arrows Mobile Fi Wi\n",
            "7 独女 たち 男性 しまう 結婚 アリ 事情 モテる 本音 悩み 女子 職場 出会い 恋愛 婚活 ホント 働く 友達 女性 理由\n",
            "8 特集 生活 ポイント オトコ 講座 見える ゴルフ わかる コラム 提案 聞く ベスト 東京 選ぶ 作る 仕事 ビジネス 持つ 効果 方法\n",
            "9 ねこ android アプリ 文字 iPhone ゲーム アクション for 画面 操作 ipad レシピ まとめる 測定 表示 デザイン 無料 note 電源 スマホ\n",
            "10 売れ筋 チェック プレーヤー USB パナソニック 電気 デジカメ 感覚 touch 電源 発売 電子 音声 電池 メーカー 撮影 デジタル ソニー 製品 シャープ\n",
            "11 選手 代表 五輪 チーム 真司 サッカー 明かす 言及 なでしこ 香川 報道 ファン アナ 絶賛 移籍 ロンドン 苦言 松井 野球 監督\n",
            "12 説教 部屋 辛口 年収 図鑑 研究 プレイヤー Vol ビジネスマン 活動 転職 会社 ソーシャル ビジネス 採用 まま 人事 メディア ウラ ベスト\n",
            "13 デジ Ubuntu Mac クラウド ソフト ダウンロード デバイス 使える 必須 テクニック PC 専用 満載 機能 裏技 情報 便利 使う レア インテル\n",
            "14 週間 ランキング ライフスタイル ビューティー お気に入り 登録 ビューティ 行動 記事 彼氏 恋愛 みんな ダイエット レシピ UP OK 方法 作る つける 解消\n",
            "15 optimus レポート HTC スペック sim 試す jojo らくらく pantone ルーター galaxy siii スマートフォン 画面 SC 全部 SO ハイ 出展 GX\n",
            "16 奇跡 バトル 描く 俳優 恐怖 スター 映画 上陸 No 誕生 ヒロイン 女優 超える 主演 セクシー 来る 成功 観る インタビュー DVD\n",
            "17 登場 ドライブ ノート 募集 イベント カード 最新 新型 商品 In 開催 公式 よる レビューアー 会議 徹底 サイト 社長 本日 結果\n",
            "18 気分 美容 クリスマス プレゼント 大人 ホテル スイーツ ファッション メイク 美肌 食べる 伝統 入れる バレンタイン 贈る 終了 カフェ ケーキ トレンド ガール\n",
            "19 salon ビデオ 連動 連載 デザイン カメラ ソニー インターネット 感覚 共有 ブラック 撮影 パナソニック 撮る 電子 未来 電池 デジカメ 動画 デジタル\n",
            "20 虎の巻 得る ファイル ワザ 知る Word PC Excel 管理 裏技 入力 ソフト IT テクニック 活用 フラッシュバック 役立つ 表示 便利 操作\n",
            "21 ソフトウェア 更新 具合 提供 IS isw NTT arrows phone KDDI AQUOS SH 起動 開始 SII ドコモ medias HT tab xperia\n",
            "22 暴露 出演 発言 母親 卒業 akb 対する 騒動 離婚 騒然 中島 連発 続出 怒り ぶり 物議 ファン エリカ 大島 やめる\n",
            "23 売れる ニュース 電池 電気 アップル 被害 テレビ 広がる 電力 電子 事故 影響 携帯 メーカー 契約 原因 ケータイ 家電 期待 書籍\n",
            "24 watch SPORTS 星野 斎藤 巨人 岡田 ダルビッシュ ノム 試合 引退 開幕 長友 監督 吉田 野球 本田 田中 真央 松井 落合\n",
            "25 過ごす 愛す 婚活 プロジェクト ひとり ケーキ オンナ モテる 食べる 男性 バレンタイン デート 入れる 女性 ブランド トレンド 結婚 クリスマス 料理 男子\n",
            "26 最近 オススメ 今週 android 画面 特集 まとめる アプリ ハイ スマホ スペック ディスプレイ モバイル galaxy note siii 測定 optimus iii スマートフォン\n",
            "27 Android ICS 予定 インチ nottv 予約 lte クアッドコア イー 向け ドコモ NTT 赤外 開始 CPU プラチナ tab 機種 防塵 ワンセグ\n",
            "28 ポスター ハリウッド 解禁 ナイト 公開 映像 予告 ダーク 主演 来日 スター ストーリー アベンジャーズ 上陸 プレミア 描く アカデミー 俳優 感動 映画\n",
            "29 presented cafe ゆるい by よめ あらわれる くさる はじめる 自転車 風呂 ひとり オンナ 教える Vol 女子 変える カフェ 健康 会える 入門\n"
          ]
        }
      ],
      "source": [
        "topic_words = []\n",
        "distances = ((centers[np.newaxis,:,:] - vocab_embeddings[:,np.newaxis,:]) ** 2).sum(-1)\n",
        "for i in range(distances.shape[-1]):\n",
        "    indices = np.argsort(distances[:,i])\n",
        "    topic_words.append(f\"{i:d} \" + \" \".join(list(vocab[indices[:20]])))\n",
        "print(\"\\n\".join(topic_words))\n",
        "#with open(\"topic_words.txt\", \"w\") as f:\n",
        "#    f.write(\"\\n\".join(topic_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BAbkeMaSUWK8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}