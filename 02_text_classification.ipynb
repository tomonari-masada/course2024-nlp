{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/course2024-nlp/blob/main/02_text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-w8jUVl2szF"
      },
      "source": [
        "# BoWによるテキスト分類\n",
        "* BoWでも良い性能を出せることが多い。\n",
        "  * LLMを使って文書分類するときは、BoW+SVMの性能と比較した方が良い。\n",
        "  * なぜなら、分類性能に大きな差がつかないことも、しばしばあるので。"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 準備"
      ],
      "metadata": {
        "id": "BBEuawhLkvNY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TU529kZR2szH"
      },
      "source": [
        "### spaCyの最小限の機能のインストール"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnIyf5Kb2szI"
      },
      "source": [
        "* 英語だけ扱えるようになる。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_97cAWK2szI"
      },
      "outputs": [],
      "source": [
        "#!pip install -U spacy\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIIy7LnT2szI"
      },
      "source": [
        "### spaCyの日本語形態素解析器のインストール\n",
        "  * Sudachiという形態素解析器が使えるようになる。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3RW3h742szJ"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download ja_core_news_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### datasetsのインストール"
      ],
      "metadata": {
        "id": "U5e5bWUzjruu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "MVfk5yJUe0sX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 乱数のシードの設定\n",
        "* transformersのset_seedを使うと便利。"
      ],
      "metadata": {
        "id": "Q_nAp4cEjbro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import set_seed\n",
        "\n",
        "set_seed(0)"
      ],
      "metadata": {
        "id": "BVUrXVJpjWlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xg1JBgXU2szJ"
      },
      "source": [
        "## データセットの取得"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ライブドアニュースコーパスのタイトルを使う。"
      ],
      "metadata": {
        "id": "H1meXE4wlMA5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ma1dUc8r2szJ"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\n",
        "    \"shunk031/livedoor-news-corpus\",\n",
        "    train_ratio=0.8,\n",
        "    val_ratio=0.1,\n",
        "    test_ratio=0.1,\n",
        "    random_state=42,\n",
        "    shuffle=True,\n",
        "    trust_remote_code=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* このsplitsをそのままディスクに保存する。"
      ],
      "metadata": {
        "id": "PVSlR6wQmRri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds.save_to_disk(\"livedoor_ds\")"
      ],
      "metadata": {
        "id": "3Px5BTuGfcjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 保存先のディレクトリを丸ごと`.tar.gz`ファイルに固める。"
      ],
      "metadata": {
        "id": "i43IPMNgmWJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tar zcvf livedoor_ds.tar.gz livedoor_ds"
      ],
      "metadata": {
        "id": "SALr9TYBft9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* データセットのsplitsを保存した`.tar.gz`ファイルを解凍する。"
      ],
      "metadata": {
        "id": "nc4g1iIKmejP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tar zxvf livedoor_ds.tar.gz"
      ],
      "metadata": {
        "id": "Er8FRtpDmG47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 解凍して出来上がったディレクトリからデータセットを読み込む。"
      ],
      "metadata": {
        "id": "UHNNDzpvmkV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_from_disk\n",
        "ds = load_from_disk(\"./livedoor_ds\")"
      ],
      "metadata": {
        "id": "3SXZ3aYUmL-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 実は記事に重複があるが、気にしないことにする。"
      ],
      "metadata": {
        "id": "vh--36XOrAUz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for content in ds[\"train\"][\"content\"]:\n",
        "  for content2 in ds[\"validation\"][\"content\"]:\n",
        "    if content == content2:\n",
        "      print(content)\n",
        "  for content2 in ds[\"test\"][\"content\"]:\n",
        "    if content == content2:\n",
        "      print(content)"
      ],
      "metadata": {
        "id": "BqXzzbHFqnHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 9種類のクラスラベルを参考までに示しておく。"
      ],
      "metadata": {
        "id": "Yb-uaLCMlONi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTBvLjgm2szJ"
      },
      "outputs": [],
      "source": [
        "category_names = [\n",
        "    'movie-enter',\n",
        "    'it-life-hack',\n",
        "    'kaden-channel',\n",
        "    'topic-news',\n",
        "    'livedoor-homme',\n",
        "    'peachy',\n",
        "    'sports-watch',\n",
        "    'dokujo-tsushin',\n",
        "    'smax',\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 内容を少し読んでみる。"
      ],
      "metadata": {
        "id": "QUR92_RSlQsJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvsgSnty2szJ"
      },
      "outputs": [],
      "source": [
        "ds[\"train\"][\"content\"][5705]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuO5V5Jw2szJ"
      },
      "source": [
        "## 形態素解析"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 一つのテキストでSudachiによる形態素解析を試してみる。"
      ],
      "metadata": {
        "id": "ZvDzC7pdfISj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dmx5x_8Q2szK"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"ja_core_news_sm\")\n",
        "doc = nlp(ds[\"train\"][\"content\"][5705])\n",
        "for token in doc:\n",
        "  print(token.lemma_, end=\" \")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 訓練データ全体を形態素解析する。\n",
        "  * TfidfVectorizerの入力として使うので・・・\n",
        "  * 形態素を空白文字でつないで一つの文字列にしておく。"
      ],
      "metadata": {
        "id": "Zh9qY80kfTp3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWXMwlwz2szK"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "corpus_train = []\n",
        "for text in tqdm(ds[\"train\"][\"content\"]):\n",
        "  doc = nlp(text)\n",
        "  text = \" \".join([token.lemma_ for token in doc])\n",
        "  for new_line_char in [\"\\n\", \"\\r\", \"\\u2028\", \"\\u2029\", \"\\u0085\"]:\n",
        "    text = \" \".join([line for line in text.split(new_line_char)])\n",
        "  corpus_train.append(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ファイルへ書き込む。"
      ],
      "metadata": {
        "id": "VJ0glnoOm2-e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UI3TGfvw2szK"
      },
      "outputs": [],
      "source": [
        "with open('livedoor_content_train.txt', 'w') as f:\n",
        "  f.write(\"\\n\".join(corpus_train) + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ファイルから読み込む。"
      ],
      "metadata": {
        "id": "lPfRfHZhm4Qz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('livedoor_content_train.txt', 'r') as f:\n",
        "  corpus_train = f.read().splitlines()"
      ],
      "metadata": {
        "id": "devh4PRrlphw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_train[0]"
      ],
      "metadata": {
        "id": "9hw63Ffwg3Ar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 検証データとテストデータも形態素解析する。"
      ],
      "metadata": {
        "id": "YmQvXd6BiIty"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WoZ8RUO2szK"
      },
      "outputs": [],
      "source": [
        "corpus_val = []\n",
        "for text in tqdm(ds[\"validation\"][\"content\"]):\n",
        "  doc = nlp(text)\n",
        "  text = \" \".join([token.lemma_ for token in doc])\n",
        "  for new_line_char in [\"\\n\", \"\\r\", \"\\u2028\", \"\\u2029\", \"\\u0085\"]:\n",
        "    text = \" \".join([line for line in text.split(new_line_char)])\n",
        "  corpus_train.append(text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('livedoor_content_val.txt', 'w') as f:\n",
        "  f.write(\"\\n\".join(corpus_val) + \"\\n\")"
      ],
      "metadata": {
        "id": "LWN19xc5obor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('livedoor_content_val.txt', 'r') as f:\n",
        "  corpus_val = f.read().splitlines()"
      ],
      "metadata": {
        "id": "4GJywA02donr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_test = []\n",
        "for text in tqdm(ds[\"test\"][\"content\"]):\n",
        "  doc = nlp(text)\n",
        "  text = \" \".join([token.lemma_ for token in doc])\n",
        "  for new_line_char in [\"\\n\", \"\\r\", \"\\u2028\", \"\\u2029\", \"\\u0085\"]:\n",
        "    text = \" \".join([line for line in text.split(new_line_char)])\n",
        "  corpus_train.append(text)"
      ],
      "metadata": {
        "id": "Dbud6eWskhTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('livedoor_content_test.txt', 'w') as f:\n",
        "  f.write(\"\\n\".join(corpus_test) + \"\\n\")"
      ],
      "metadata": {
        "id": "b8WzFXQHodGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('livedoor_content_test.txt', 'r') as f:\n",
        "  corpus_test = f.read().splitlines()"
      ],
      "metadata": {
        "id": "ZEY-1YZudrvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEDViFJg2szK"
      },
      "source": [
        "## TF-IDFベクトルの計算"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azmYC6HJ2szK"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(min_df=10, max_df=0.2)\n",
        "X_train = vectorizer.fit_transform(corpus_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "O0soXLPUmRtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 課題\n",
        "* ライブドアニュースコーパスを9値分類する分類器を作ろう。\n",
        "* 注意：データセットの分割の仕方は変えないようにしましょう。"
      ],
      "metadata": {
        "id": "fmStHVX0hej3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 実行例"
      ],
      "metadata": {
        "id": "b3s8109Mh5hY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 交差検証でハイパーパラメータのチューニングをする。"
      ],
      "metadata": {
        "id": "YUOyJkfrveA-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgTwGYuo2szK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "corpus = np.array(corpus_train + corpus_val)\n",
        "labels = np.array(ds[\"train\"][\"category\"] + ds[\"validation\"][\"category\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qim4TH6A2szK"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1234)\n",
        "\n",
        "for min_df in [10, 20, 30]:\n",
        "  for max_df in [0.2, 0.3, 0.4]:\n",
        "    vectorizer = TfidfVectorizer(min_df=min_df, max_df=max_df)\n",
        "    for C in 10. ** np.arange(-1, 4):\n",
        "      scores = []\n",
        "      skf_split = skf.split(corpus, labels)\n",
        "      for train_index, val_index in skf_split:\n",
        "        X_train = vectorizer.fit_transform(corpus[train_index])\n",
        "        clf = LinearSVC(C=C, dual=False, max_iter=1000, random_state=123)\n",
        "        clf.fit(X_train, labels[train_index])\n",
        "        X_val = vectorizer.transform(corpus[val_index])\n",
        "        score = clf.score(X_val, labels[val_index])\n",
        "        print(f\"\\t{score:.3f}\", end=\" \")\n",
        "        scores.append(score)\n",
        "      print(f\"\\nmean accuracy: {np.array(scores).mean():.3f}\", end=\"\")\n",
        "      print(f\" | C={C:.2e} min_df={min_df} max_df={max_df:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QShaXKTL2szK"
      },
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer(min_df=20)\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "clf = LinearSVC(C=10.0, dual=False, max_iter=1000, random_state=123)\n",
        "clf.fit(X, labels)\n",
        "X_test = vectorizer.transform(corpus_test)\n",
        "score = clf.score(X_test, ds[\"test\"][\"category\"])\n",
        "print(f\"{score:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SsH2RFTwisns"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}