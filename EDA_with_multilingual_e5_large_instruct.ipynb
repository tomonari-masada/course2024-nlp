{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/course2024-nlp/blob/main/EDA_with_multilingual_e5_large_instruct.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfXkyxsBHKmd"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.functional import normalize\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    TrainingArguments,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        ")\n",
        "from transformers.modeling_outputs import ModelOutput\n",
        "\n",
        "from trl import SFTTrainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jp9BTnJaHKme"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\n",
        "    \"shunk031/livedoor-news-corpus\",\n",
        "    train_ratio=0.8, val_ratio=0.1, test_ratio=0.1,\n",
        "    random_state=42,\n",
        "    shuffle=True,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "num_categories = len(set(dataset[\"train\"][\"category\"]))\n",
        "max_seq_length = 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOE3aClgHKme"
      },
      "outputs": [],
      "source": [
        "category_names = ['movie-enter', 'it-life-hack', 'kaden-channel', 'topic-news', 'livedoor-homme', 'peachy', 'sports-watch', 'dokujo-tsushin', 'smax']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OG2l9ms-HKmf"
      },
      "outputs": [],
      "source": [
        "model_id = \"intfloat/multilingual-e5-large-instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, max_seq_length=max_seq_length)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_id,\n",
        "    num_labels=num_categories,\n",
        ").to(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sd1D8x0AHKmf"
      },
      "outputs": [],
      "source": [
        "def accuracy(model, tokenizer, corpus, labels, batch_size=4):\n",
        "    model.eval()\n",
        "    num_correct_answers, num_answers = 0, 0\n",
        "    for i in tqdm(range(0, len(corpus), batch_size)):\n",
        "        texts = corpus[i:i+batch_size]\n",
        "        encodings = tokenizer(texts, padding=True, return_tensors=\"pt\")\n",
        "        encodings = encodings.to(model.device)\n",
        "        category = torch.tensor(labels[i:i+batch_size]).to(model.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**encodings)\n",
        "        predicted = outputs.logits.argmax(-1)\n",
        "        num_correct_answers += (predicted == category).sum()\n",
        "        num_answers += len(texts)\n",
        "    model.train()\n",
        "    return (num_correct_answers / num_answers).item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ee--_HGtHKmf"
      },
      "outputs": [],
      "source": [
        "def average_pool(last_hidden_states, attention_mask):\n",
        "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
        "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjFN029MHKmg"
      },
      "outputs": [],
      "source": [
        "def embed(model, tokenizer, corpus, batch_size=4):\n",
        "    model.eval()\n",
        "    pooled_hidden_states = []\n",
        "    for i in tqdm(range(0, len(corpus), batch_size)):\n",
        "        texts = corpus[i:i+batch_size]\n",
        "        encodings = tokenizer(texts, padding=True, return_tensors=\"pt\")\n",
        "        encodings = encodings.to(model.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model.roberta(**encodings)\n",
        "        pooled_hidden_state = average_pool(\n",
        "            outputs.last_hidden_state,\n",
        "            encodings['attention_mask'],\n",
        "        )\n",
        "        pooled_hidden_states.append(pooled_hidden_state.cpu())\n",
        "    model.train()\n",
        "    return torch.cat(pooled_hidden_states)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCNFsD-FHKmg"
      },
      "outputs": [],
      "source": [
        "embeddings = {}\n",
        "for key in dataset:\n",
        "    embeddings[key] = embed(model, tokenizer, dataset[key][\"title\"])\n",
        "    embeddings[key] = normalize(embeddings[key])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqVm7LOaHKmg"
      },
      "outputs": [],
      "source": [
        "n_clusters = 30\n",
        "kmeans = KMeans(n_clusters=n_clusters, n_init='auto', random_state=123)\n",
        "kmeans.fit(embeddings[\"train\"])\n",
        "centers = kmeans.cluster_centers_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Aa4cPwNHKmh"
      },
      "outputs": [],
      "source": [
        "unique, counts = np.unique(kmeans.labels_, return_counts=True)\n",
        "size_dict = dict(zip(unique, counts))\n",
        "print(sorted([item[1] for item in size_dict.items()]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRuaYjpfHKmh"
      },
      "outputs": [],
      "source": [
        "label_pos_tags = [\"NOUN\", \"VERB\", \"PROPN\"]\n",
        "\n",
        "nlp = spacy.load(\"ja_core_news_sm\")\n",
        "corpus = {}\n",
        "for key in dataset:\n",
        "    corpus[key] = []\n",
        "    for text in tqdm(dataset[key][\"title\"]):\n",
        "        corpus[key].append(\" \".join(\n",
        "            [token.lemma_\n",
        "             for token in nlp(text) if token.pos_ in label_pos_tags\n",
        "            ]\n",
        "        ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgeSTmazHKmh"
      },
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer(min_df=10, max_df=0.1, lowercase=False)\n",
        "vectorizer.fit(corpus[\"train\"])\n",
        "vocab = np.array(vectorizer.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utC1WgrNHKmh"
      },
      "outputs": [],
      "source": [
        "vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pG9W3c9VHKmh"
      },
      "outputs": [],
      "source": [
        "vocab_embeddings = embed(model, tokenizer, list(vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJm6NCZ1HKmh"
      },
      "outputs": [],
      "source": [
        "topic_words = []\n",
        "similarities = cosine_similarity(vocab_embeddings, centers)\n",
        "for i in range(similarities.shape[-1]):\n",
        "    indices = np.argsort(- similarities[:,i])\n",
        "    topic_words.append(f\"{i:d} \" + \" \".join(list(vocab[indices[:20]])))\n",
        "print(\"\\n\".join(topic_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMUSkFxFHKmh"
      },
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer(min_df=10, max_df=0.1, lowercase=False)\n",
        "vectorizer.fit(corpus[\"train\"])\n",
        "vocab = np.array(vectorizer.get_feature_names_out())\n",
        "X_train = vectorizer.transform(corpus[\"train\"]).toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAvHgqTKHKmi"
      },
      "outputs": [],
      "source": [
        "vocab_embeddings = np.dot((X_train / X_train.sum(0)).T, embeddings[\"train\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sJmYQXaHKmi"
      },
      "outputs": [],
      "source": [
        "topic_words = []\n",
        "similarities = cosine_similarity(vocab_embeddings, centers)\n",
        "for i in range(similarities.shape[-1]):\n",
        "    indices = np.argsort(- similarities[:,i])\n",
        "    topic_words.append(f\"{i:d} \" + \" \".join(list(vocab[indices[:20]])))\n",
        "print(\"\\n\".join(topic_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7BiTxO0HKmi"
      },
      "outputs": [],
      "source": [
        "class MyNetForClassification(nn.Module):\n",
        "    def __init__(self, pretrained):\n",
        "        super().__init__()\n",
        "        self.pretrained = pretrained\n",
        "        self.config = self.pretrained.config\n",
        "\n",
        "    def forward(\n",
        "        self, input_ids, category=None,\n",
        "        attention_mask=None,\n",
        "        output_attentions=None, output_hidden_states=None,\n",
        "        return_dict=None, inputs_embeds=None, labels=None,\n",
        "    ):\n",
        "        outputs = self.pretrained(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        loss_fct = nn.CrossEntropyLoss()\n",
        "        loss = loss_fct(outputs.logits, category)\n",
        "        return ModelOutput(\n",
        "            loss=loss,\n",
        "            logits=outputs.logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "my_model = MyNetForClassification(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cbnp0A1HKmi"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=8,\n",
        "    output_dir=\"outputs\",\n",
        "    label_names=[\"category\"],\n",
        "    max_steps=300,\n",
        "    eval_steps=100,\n",
        "    logging_steps=100,\n",
        "    save_steps=100,\n",
        "    learning_rate=5e-5,\n",
        "    optim_target_modules=[\"query\", \"key\", \"value\", \"dense\"],\n",
        "    evaluation_strategy=\"steps\",\n",
        "    logging_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    load_best_model_at_end=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WljjKg7rHKmi"
      },
      "outputs": [],
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=my_model,\n",
        "    args=training_args,\n",
        "    tokenizer=tokenizer,\n",
        "    max_seq_length=max_seq_length,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"validation\"],\n",
        "    dataset_text_field=\"title\",\n",
        ")\n",
        "trainer.train_dataset = trainer.train_dataset.add_column(\n",
        "    \"category\", dataset[\"train\"][\"category\"],\n",
        ")\n",
        "trainer.eval_dataset = trainer.eval_dataset.add_column(\n",
        "    \"category\", dataset[\"validation\"][\"category\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFD8qR9AHKmi"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PkloiqaHHKmi"
      },
      "outputs": [],
      "source": [
        "accuracy(model, tokenizer, dataset[\"validation\"][\"title\"], dataset[\"validation\"][\"category\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjNUewBGHKmi"
      },
      "outputs": [],
      "source": [
        "embeddings = {}\n",
        "for key in dataset:\n",
        "    embeddings[key] = embed(model, tokenizer, dataset[key][\"title\"])\n",
        "    embeddings[key] = normalize(embeddings[key])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWXvFlGBHKmj"
      },
      "outputs": [],
      "source": [
        "vocab_embeddings = np.dot((X_train / X_train.sum(0)).T, embeddings[\"train\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6K08fjDHKmj"
      },
      "outputs": [],
      "source": [
        "n_clusters = 30\n",
        "kmeans = KMeans(n_clusters=n_clusters, n_init='auto', random_state=123)\n",
        "kmeans.fit(embeddings[\"train\"])\n",
        "centers = kmeans.cluster_centers_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWue6TW8HKmj"
      },
      "outputs": [],
      "source": [
        "unique, counts = np.unique(kmeans.labels_, return_counts=True)\n",
        "size_dict = dict(zip(unique, counts))\n",
        "print(sorted([item[1] for item in size_dict.items()]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rs4n13cHKmj"
      },
      "outputs": [],
      "source": [
        "topic_words = []\n",
        "similarities = cosine_similarity(vocab_embeddings, centers)\n",
        "for i in range(similarities.shape[-1]):\n",
        "    indices = np.argsort(- similarities[:,i])\n",
        "    topic_words.append(f\"{i:d} \" + \" \".join(list(vocab[indices[:20]])))\n",
        "print(\"\\n\".join(topic_words))\n",
        "#with open(\"topic_words.txt\", \"w\") as f:\n",
        "#    f.write(\"\\n\".join(topic_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAkY-lFcHKmj"
      },
      "outputs": [],
      "source": [
        "for param in model.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yoI31tWnHKmj"
      },
      "outputs": [],
      "source": [
        "model.to(\"cpu\").eval();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdiM-ImpHKmj"
      },
      "outputs": [],
      "source": [
        "from captum.attr import LayerIntegratedGradients, TokenReferenceBase, visualization\n",
        "\n",
        "token_reference = TokenReferenceBase(reference_token_idx=tokenizer.pad_token_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lt9lSIEEHKmj"
      },
      "outputs": [],
      "source": [
        "text = dataset[\"train\"][\"title\"][0]\n",
        "encodings = tokenizer(text, padding=True, return_tensors=\"pt\")\n",
        "encodings = encodings.to(model.device)\n",
        "outputs = model.roberta(encodings.input_ids, encodings.attention_mask)\n",
        "pooled_hidden_state = average_pool(\n",
        "    outputs.last_hidden_state,\n",
        "    encodings['attention_mask'],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLk7c4yzHKmj"
      },
      "outputs": [],
      "source": [
        "pooled_hidden_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IO_Q-OaDHKmj"
      },
      "outputs": [],
      "source": [
        "cluster_centers = torch.tensor(kmeans.cluster_centers_, device=model.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yV64yVgHHKmk"
      },
      "outputs": [],
      "source": [
        "cos_sim = nn.CosineSimilarity(dim=-1)\n",
        "cos_sim(cluster_centers, pooled_hidden_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4GVNe0iHKmk"
      },
      "outputs": [],
      "source": [
        "kmeans.labels_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aj2qoUESHKmk"
      },
      "outputs": [],
      "source": [
        "def predict(input_ids, attention_mask):\n",
        "    outputs = model.roberta(input_ids, attention_mask)\n",
        "    pooled_hidden_state = average_pool(\n",
        "        outputs.last_hidden_state,\n",
        "        attention_mask,\n",
        "    )\n",
        "    return cos_sim(\n",
        "        cluster_centers.unsqueeze(0),\n",
        "        pooled_hidden_state.unsqueeze(1)\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLL67X_0HKmk"
      },
      "outputs": [],
      "source": [
        "text = dataset[\"train\"][\"title\"][0]\n",
        "encodings = tokenizer(text, padding=True, return_tensors=\"pt\")\n",
        "encodings.to(model.device)\n",
        "predict(\n",
        "    encodings.input_ids,\n",
        "    encodings.attention_mask,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGH5vrpaHKmk"
      },
      "outputs": [],
      "source": [
        "text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-3EhnV5HKmn"
      },
      "outputs": [],
      "source": [
        "def cluster_similarity_forward_func(input_ids, attention_mask, cluster_id):\n",
        "    similarities = predict(input_ids, attention_mask)\n",
        "    return similarities[:,cluster_id]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tlwn_BgQHKmn"
      },
      "outputs": [],
      "source": [
        "text = dataset[\"train\"][\"title\"][0]\n",
        "encodings = tokenizer(text, padding=True, return_tensors=\"pt\")\n",
        "encodings.to(model.device)\n",
        "cluster_similarity_forward_func(\n",
        "    encodings.input_ids,\n",
        "    encodings.attention_mask,\n",
        "    29,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ql4t3y-kHKmn"
      },
      "outputs": [],
      "source": [
        "lig = LayerIntegratedGradients(\n",
        "    cluster_similarity_forward_func,\n",
        "    model.roberta.embeddings.word_embeddings,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5Do27kaHKmn"
      },
      "outputs": [],
      "source": [
        "vis_data_records_ig = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwJohXipHKmn"
      },
      "outputs": [],
      "source": [
        "def add_attributions_to_visualizer(attributions, text, pred_prob, pred_class, true_class,\n",
        "                                   attr_class, convergence_scores, vis_data_records):\n",
        "    attributions = attributions.cpu()\n",
        "    attributions = attributions.sum(dim=-1).squeeze(0)\n",
        "    attributions = attributions / torch.norm(attributions)\n",
        "    attributions = attributions.cpu().detach().numpy()\n",
        "    vis_data_records.append(\n",
        "        visualization.VisualizationDataRecord(\n",
        "            attributions,\n",
        "            pred_prob,\n",
        "            pred_class,\n",
        "            true_class,\n",
        "            attr_class,\n",
        "            attributions.sum(),\n",
        "            text,\n",
        "            convergence_scores,\n",
        "        )\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcyCOiOJHKmo"
      },
      "outputs": [],
      "source": [
        "def interpret_text(text, attr_class=None, n_steps=50):\n",
        "    encodings = tokenizer(text, padding=True, return_tensors=\"pt\")\n",
        "    encodings = encodings.to(model.device)\n",
        "    input_ids = encodings.input_ids\n",
        "    attention_mask = encodings.attention_mask\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
        "    reference_input_ids = token_reference.generate_reference(\n",
        "        len(tokens),\n",
        "        device=model.device,\n",
        "    ).unsqueeze(0)\n",
        "\n",
        "    similarities = predict(\n",
        "        input_ids,\n",
        "        attention_mask,\n",
        "    )\n",
        "    prediction = similarities.argmax().item()\n",
        "    if attr_class is None:\n",
        "        attr_class = prediction\n",
        "    print(\n",
        "        f\"prediction={prediction} \"\n",
        "        f\"cos_sim={similarities.max().item():.3f} \",\n",
        "        end=\"\"\n",
        "    )\n",
        "\n",
        "    attributions_ig, delta = lig.attribute(\n",
        "        input_ids,\n",
        "        reference_input_ids,\n",
        "        additional_forward_args=(attention_mask, attr_class),\n",
        "        n_steps=n_steps,\n",
        "        return_convergence_delta=True,\n",
        "    )\n",
        "    print(f\"convergence delta={delta.item():.3e} when n_steps={n_steps}\")\n",
        "\n",
        "    add_attributions_to_visualizer(\n",
        "        attributions_ig,\n",
        "        tokens,\n",
        "        similarities.max().item(),\n",
        "        str(prediction),\n",
        "        str(prediction),\n",
        "        str(attr_class),\n",
        "        delta,\n",
        "        vis_data_records_ig,\n",
        "    )\n",
        "    return prediction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pr8szNeHHKmo"
      },
      "outputs": [],
      "source": [
        "vis_data_records_ig = []\n",
        "for n_steps in [50, 100, 200, 300]:\n",
        "    interpret_text(dataset[\"train\"][\"title\"][0], n_steps=n_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBTlEOryHKmo"
      },
      "outputs": [],
      "source": [
        "visualization.visualize_text(vis_data_records_ig);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOGMcYBZHKmo"
      },
      "outputs": [],
      "source": [
        "for i in tqdm(range(60, 70)):\n",
        "    example = dataset[\"validation\"][i]\n",
        "    print(category_names[example[\"category\"]], end=\" \")\n",
        "    vis_data_records_ig = []\n",
        "    prediction = interpret_text(example[\"title\"], n_steps=50)\n",
        "    print(\"\\t\" + topic_words[prediction])\n",
        "    visualization.visualize_text(vis_data_records_ig);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PP_9FwOjHKmo"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}