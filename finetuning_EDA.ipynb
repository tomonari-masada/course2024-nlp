{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/course2024-nlp/blob/main/finetuning_EDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDmt9KWo6HtB"
      },
      "source": [
        "## 生成モデルをデータ分析に使う"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1fhNVeL6HtC"
      },
      "source": [
        "### （実演） トピック抽出にLLMを使う\n",
        "* 与えられたテキスト集合から、指定した個数のトピックを抽出する。\n",
        "  * 各トピックは、そのトピックに属するテキストの集合として表される。\n",
        "  * また、各トピックの内容を、20個の単語リストで表す。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2i0F-Mkr6HtC"
      },
      "source": [
        "### 準備"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "peFsVRI-6HtC"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download ja_core_news_sm\n",
        "#!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install -U transformers==4.40\n",
        "!pip install -U bitsandbytes accelerate peft trl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLM_tnKZ6HtD"
      },
      "source": [
        "### インポート"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFUXG6jz6HtD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "from transformers import (\n",
        "    set_seed,\n",
        "    BitsAndBytesConfig,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "from transformers.modeling_outputs import ModelOutput\n",
        "from peft import LoraConfig, PeftModel\n",
        "from trl import SFTTrainer\n",
        "\n",
        "set_seed(123)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Imb-gF_T6HtD"
      },
      "source": [
        "### データセット\n",
        "* livedoorニュースコーパス\n",
        "  * ９つのジャンルのニュース記事からなるテキストの集合。\n",
        "  * ニュース記事のタイトル（短いテキスト）と内容（やや長いテキスト）からなる。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5aUeaF46HtE"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\n",
        "    \"shunk031/livedoor-news-corpus\",\n",
        "    train_ratio=0.8,\n",
        "    val_ratio=0.1,\n",
        "    test_ratio=0.1,\n",
        "    random_state=42,\n",
        "    shuffle=True,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "num_categories = len(set(dataset[\"train\"][\"category\"]))\n",
        "\n",
        "max_seq_length = 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXozROMm6HtE"
      },
      "outputs": [],
      "source": [
        "dataset[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7rwu1516HtE"
      },
      "source": [
        "### モデルのロード\n",
        "* `elyza/ELYZA-japanese-Llama-2-7b`というllama 2ベースの日本語対応LLMを使う。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Scnpf52f6HtE"
      },
      "outputs": [],
      "source": [
        "model_name = \"elyza/ELYZA-japanese-Llama-2-7b\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_storage=torch.bfloat16,\n",
        ")\n",
        "\n",
        "pretrained = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=num_categories,\n",
        "    quantization_config=bnb_config,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    low_cpu_mem_usage=True,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, max_seq_length=max_seq_length)\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "pretrained.config.pad_token_id = pretrained.config.eos_token_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3Te7ob36HtF"
      },
      "source": [
        "* 分類の損失を自前で計算するため、新たにクラスを定義する。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgxbVQwq6HtF"
      },
      "outputs": [],
      "source": [
        "class LivedoorNet(nn.Module):\n",
        "    def __init__(self, pretrained):\n",
        "        super().__init__()\n",
        "        self.pretrained = pretrained\n",
        "        self.config = self.pretrained.config\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids,\n",
        "        category=None,\n",
        "        attention_mask=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "    ):\n",
        "        outputs = self.pretrained(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        loss_fct = nn.CrossEntropyLoss()\n",
        "        loss = loss_fct(outputs.logits, category)\n",
        "        return ModelOutput(\n",
        "            loss=loss,\n",
        "            logits=outputs.logits,\n",
        "            past_key_values=outputs.past_key_values,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "\n",
        "model = LivedoorNet(pretrained)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4LN0Zm46HtF"
      },
      "source": [
        "### LoRAの設定"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zaptblE06HtF"
      },
      "outputs": [],
      "source": [
        "peft_config = LoraConfig(\n",
        "    r=32,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"SEQ_CLS\",\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "        ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9shlgRM6HtF"
      },
      "source": [
        "* finetuningは実行せず、保存してあるLoRAを読み込むときは、以下のセルを実行する。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCooG_mP6HtF"
      },
      "outputs": [],
      "source": [
        "model = PeftModel.from_pretrained(model, \"models/lora/\" + model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqreLplH6HtF"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    output_dir=\"outputs_cls\",\n",
        "    label_names=[\"category\"],\n",
        "    max_steps=500,\n",
        "    eval_steps=100,\n",
        "    logging_steps=100,\n",
        "    save_steps=100,\n",
        "    learning_rate=5e-5,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    logging_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    load_best_model_at_end=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "woo8DPYn6HtG"
      },
      "outputs": [],
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    tokenizer=tokenizer,\n",
        "    max_seq_length=max_seq_length,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"validation\"],\n",
        "    dataset_text_field=\"title\",\n",
        "    peft_config=peft_config,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOQW3BnA6HtG"
      },
      "source": [
        "* 以下の作業が必要。\n",
        "  * Trainerのインスタンスを作るとtokenizationに無関係なfieldは削除されてしまう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAAK2rsC6HtG"
      },
      "outputs": [],
      "source": [
        "trainer.train_dataset = trainer.train_dataset.add_column(\"category\", dataset[\"train\"][\"category\"])\n",
        "trainer.eval_dataset = trainer.eval_dataset.add_column(\"category\", dataset[\"validation\"][\"category\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uw2HM7BT6HtG"
      },
      "source": [
        "* 評価用のヘルパ関数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5-B3g2v6HtG"
      },
      "outputs": [],
      "source": [
        "def accuracy(trainer, dataset, batch_size=4):\n",
        "    trainer.model.eval()\n",
        "    num_correct_answers = 0\n",
        "    num_answers = 0\n",
        "    for i in tqdm(range(0, len(dataset), batch_size)):\n",
        "        examples = dataset[i:i+batch_size]\n",
        "        encodings = trainer.tokenizer(\n",
        "            examples[\"title\"],\n",
        "            padding=True,\n",
        "            return_tensors=\"pt\",\n",
        "            )\n",
        "        category = torch.tensor(examples[\"category\"])\n",
        "        with torch.no_grad():\n",
        "            outputs = trainer.model(**encodings, category=category)\n",
        "        num_correct_answers += (outputs.logits.argmax(-1) == category).sum()\n",
        "        num_answers += len(examples[\"category\"])\n",
        "    trainer.model.train()\n",
        "    return num_correct_answers / num_answers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJxgP8jH6HtG"
      },
      "source": [
        "* finetuning前に分類性能を正解率で評価"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UkegbQn6HtG"
      },
      "outputs": [],
      "source": [
        "accuracy(trainer, dataset[\"validation\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iK_lWLHz6HtG"
      },
      "source": [
        "* fine-tuningの実行\n",
        "  * この実行例で必要なGPUのメモリは10GB未満。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpH0JRqL6HtG"
      },
      "outputs": [],
      "source": [
        "#trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TqZSG3i6HtG"
      },
      "outputs": [],
      "source": [
        "#accuracy(trainer, dataset[\"validation\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4W1dq_I6HtH"
      },
      "outputs": [],
      "source": [
        "#trainer.model.save_pretrained(\"models/lora/\" + model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmuTfo7q6HtH"
      },
      "source": [
        "### テキストの埋め込みを求めるヘルパ関数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gb9CVg16HtH"
      },
      "outputs": [],
      "source": [
        "def embed(trainer, dataset, batch_size=4):\n",
        "    trainer.model.eval()\n",
        "    categories = []\n",
        "    pooled_hidden_states = []\n",
        "    for i in tqdm(range(0, len(dataset), batch_size)):\n",
        "        examples = dataset[i:i+batch_size]\n",
        "        encodings = trainer.tokenizer(\n",
        "            examples[\"title\"],\n",
        "            padding=True,\n",
        "            return_tensors=\"pt\",\n",
        "            )\n",
        "        categories += list(examples[\"category\"])\n",
        "        with torch.no_grad():\n",
        "            outputs = trainer.model.pretrained.model(**encodings)\n",
        "        pad_token_id = trainer.model.pretrained.config.pad_token_id\n",
        "        input_ids = encodings.input_ids\n",
        "        sequence_lengths = torch.eq(input_ids, pad_token_id).int().argmax(-1) - 1\n",
        "        sequence_lengths = sequence_lengths % input_ids.shape[-1]\n",
        "        temp_batch_size = input_ids.shape[0]\n",
        "        pooled_hidden_state = outputs.last_hidden_state[\n",
        "            torch.arange(temp_batch_size, device=outputs.last_hidden_state.device),\n",
        "            sequence_lengths]\n",
        "        pooled_hidden_states.append(pooled_hidden_state.float().cpu().numpy())\n",
        "    trainer.model.train()\n",
        "    return categories, np.concatenate(pooled_hidden_states)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbtk55Sr6HtH"
      },
      "source": [
        "### 全テキストの埋め込み"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBbRT4IS6HtH"
      },
      "outputs": [],
      "source": [
        "full_dataset = concatenate_datasets([dataset[\"train\"], dataset[\"validation\"], dataset[\"test\"]])\n",
        "categories, embeddings = embed(trainer, full_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNzrC2TX6HtH"
      },
      "outputs": [],
      "source": [
        "embeddings.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5Vfe9ue6HtH"
      },
      "source": [
        "### クラスタのラベルとして使う語彙の作成\n",
        "* spaCyで記事の全タイトルを形態素解析する。\n",
        "  * 名詞、動詞、固有名詞だけを残す。動詞は原形に直す。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MI_yzff56HtH"
      },
      "outputs": [],
      "source": [
        "label_pos_tags = [\"NOUN\", \"VERB\", \"PROPN\"]\n",
        "\n",
        "nlp = spacy.load(\"ja_core_news_sm\")\n",
        "corpus = []\n",
        "for text in tqdm(full_dataset[\"title\"]):\n",
        "    corpus.append(\" \".join([token.lemma_ for token in nlp(text) if token.pos_ in label_pos_tags]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MXTDyKc6HtH"
      },
      "source": [
        "### TF-IDFの計算\n",
        "  * `TfidfVectorizer`の`min_df`パラメータは適当に調節する。\n",
        "  * クラスタのラベリングに向かないマイナーな単語が含まれないようにする。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAgJZWlS6HtH"
      },
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer(min_df=10, lowercase=False)\n",
        "X = vectorizer.fit_transform(corpus).toarray()\n",
        "vocab = np.array(vectorizer.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jGAuy1V6HtK"
      },
      "source": [
        "### クラスタのラベルとして使う語彙の埋め込み"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmrR4Pq66HtL"
      },
      "outputs": [],
      "source": [
        "vocab_embeddings = np.dot((X / X.sum(0)).T, embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4q-Ifbd6HtL"
      },
      "source": [
        "### テキストのクラスタリング"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0VcXo_o6HtL"
      },
      "source": [
        "* クラスタ数は適当に決める。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOp9mium6HtL"
      },
      "outputs": [],
      "source": [
        "n_clusters = 20\n",
        "kmeans = KMeans(n_clusters=n_clusters, n_init='auto', random_state=123)\n",
        "kmeans.fit(embeddings)\n",
        "centers = kmeans.cluster_centers_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sn_zf0uy6HtL"
      },
      "source": [
        "* クラスタのサイズの分布を調べる。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXDD640P6HtL"
      },
      "outputs": [],
      "source": [
        "unique, counts = np.unique(kmeans.labels_, return_counts=True)\n",
        "size_dict = dict(zip(unique, counts))\n",
        "print(size_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n31cKkdZ6HtL"
      },
      "source": [
        "### 各クラスタの重心に近い順に20個の単語を列挙\n",
        "* これが各クラスタのラベルになる。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jvpKldM6HtL"
      },
      "outputs": [],
      "source": [
        "similarities = cosine_similarity(vocab_embeddings, centers)\n",
        "\n",
        "for i in range(similarities.shape[-1]):\n",
        "    indices = np.argsort(- similarities[:,i])\n",
        "    print(\" \".join(list(vocab[indices[:20]])))\n",
        "    print(\"-\"*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbECceoh6HtL"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}