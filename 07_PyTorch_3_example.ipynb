{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/course2024-nlp/blob/main/07_PyTorch_3_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbjXLZStpYp9"
      },
      "outputs": [],
      "source": [
        "!pip install datasets tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EecUGZKspEal"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from tokenizers import Tokenizer, normalizers\n",
        "from tokenizers.models import WordPiece\n",
        "from tokenizers.normalizers import NFD, Lowercase, StripAccents\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.trainers import WordPieceTrainer\n",
        "\n",
        "\n",
        "def set_seed(seed):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(0)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7p6LtyekseI",
        "outputId": "7ff23b57-7b95-4d46-9842-0d470d9d79ae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 114000\n",
              "    })\n",
              "    valid: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 6000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 7600\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset = load_dataset(\"ag_news\")\n",
        "ag_news_label = { 0: \"World\", 1: \"Sports\", 2: \"Business\", 3: \"Sci/Tec\" }\n",
        "\n",
        "train_valid = dataset[\"train\"].train_test_split(test_size=0.05)\n",
        "dataset = DatasetDict({\n",
        "    \"train\": train_valid[\"train\"],\n",
        "    \"valid\": train_valid[\"test\"],\n",
        "    \"test\": dataset[\"test\"],\n",
        "})\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JdtZhLnpF5Gs",
        "outputId": "63a345ac-e770-4ab5-9c55-aaee48af6fac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
        "tokenizer.normalizer = normalizers.Sequence([NFD(), Lowercase(), StripAccents()])\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "trainer = WordPieceTrainer(special_tokens=[\"[UNK]\"])\n",
        "tokenizer.train_from_iterator(dataset[\"train\"][\"text\"], trainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDZvdhK3cPb5"
      },
      "outputs": [],
      "source": [
        "vocab = tokenizer.get_vocab()\n",
        "print([k for k, v in sorted(vocab.items(), key=lambda item: item[1])])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRrfbx4W4T2n"
      },
      "outputs": [],
      "source": [
        "def collate_batch(batch):\n",
        "  label_list, text_list, offsets = [], [], [0]\n",
        "  for instance in batch:\n",
        "    _label, _text = instance[\"label\"], instance[\"text\"]\n",
        "    label_list.append(_label)\n",
        "    token_ids = torch.tensor(tokenizer.encode(_text).ids, dtype=torch.int64)\n",
        "    text_list.append(token_ids)\n",
        "    offsets.append(token_ids.size(0))\n",
        "  label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "  offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "  text_list = torch.cat(text_list)\n",
        "  return label_list.to(device), text_list.to(device), offsets.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQ-6jH0h4E-1"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "train_dataloader = DataLoader(dataset[\"train\"], batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "valid_dataloader = DataLoader(dataset[\"valid\"], batch_size=BATCH_SIZE, collate_fn=collate_batch)\n",
        "test_dataloader = DataLoader(dataset[\"test\"], batch_size=BATCH_SIZE, collate_fn=collate_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRJysWR_r9cW"
      },
      "outputs": [],
      "source": [
        "class TextClassificationModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size, embed_dim, num_class):\n",
        "    super(TextClassificationModel, self).__init__()\n",
        "    self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)\n",
        "    self.fc1 = nn.Linear(embed_dim, embed_dim)\n",
        "    self.fc2 = nn.Linear(embed_dim, embed_dim)\n",
        "    self.fc3 = nn.Linear(embed_dim, num_class)\n",
        "    self.act = nn.ReLU()\n",
        "    self.dropout = nn.Dropout()\n",
        "\n",
        "  def forward(self, text, offsets):\n",
        "    embedded = self.dropout(self.embedding(text, offsets))\n",
        "    hidden = self.act(self.fc1(embedded)) + embedded\n",
        "    hidden = self.act(self.fc2(hidden)) + hidden\n",
        "    return self.fc3(hidden)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4kpm5Ie7tMN",
        "outputId": "cc49d2ea-76b7-496e-f5dc-bde7bc936683"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{0, 1, 2, 3}\n"
          ]
        }
      ],
      "source": [
        "unique_labels = set([label for label in dataset[\"train\"][\"label\"]])\n",
        "print(unique_labels)\n",
        "num_class = len(unique_labels)\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "emsize = 128\n",
        "\n",
        "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QD5aA7I7cPb7"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-3\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BeGMjdPd7dXM"
      },
      "outputs": [],
      "source": [
        "def train(dataloader):\n",
        "  model.train()\n",
        "  total_acc, total_count = 0, 0\n",
        "  start_time = time.time()\n",
        "  for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "    optimizer.zero_grad()\n",
        "    predicted_label = model(text, offsets)\n",
        "    loss = criterion(predicted_label, label)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
        "    total_count += label.size(0)\n",
        "  elapsed = time.time() - start_time\n",
        "  return total_acc / total_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8JEgh9O3sAPj"
      },
      "outputs": [],
      "source": [
        "def evaluate(dataloader):\n",
        "  model.eval()\n",
        "  total_acc, total_count = 0, 0\n",
        "  with torch.no_grad():\n",
        "    for _, (label, text, offsets) in enumerate(dataloader):\n",
        "      predicted_label = model(text, offsets)\n",
        "      total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
        "      total_count += label.size(0)\n",
        "  return total_acc / total_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vixuo6u2r__K"
      },
      "outputs": [],
      "source": [
        "epochs = 20\n",
        "\n",
        "total_accu = None\n",
        "start_time = time.time()\n",
        "for epoch in range(epochs):\n",
        "  accu_train = train(train_dataloader)\n",
        "  accu_val = evaluate(valid_dataloader)\n",
        "  if total_accu is not None and total_accu > accu_val:\n",
        "    scheduler.step()\n",
        "  total_accu = accu_val\n",
        "  elapsed = time.time() - start_time\n",
        "  print(\n",
        "      f\"| epoch {epoch+1} ({elapsed:.2f}s) | \"\n",
        "      f\"lr={optimizer.param_groups[0]['lr']:.2e} | \"\n",
        "      f\"train accu {accu_train:.3f} | \"\n",
        "      f\"val accu {accu_val:.3f}\"\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o45KtyPZsF68",
        "outputId": "7c005630-7d43-41ed-b50d-9dc9cc097015"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test accuracy    0.928\n"
          ]
        }
      ],
      "source": [
        "accu_test = evaluate(test_dataloader)\n",
        "print(f\"test accuracy {accu_test:8.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FMoLIJ5sF3S",
        "outputId": "3131b8c2-fe0e-4aa5-f1da-968d08adb1fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This is a Business news\n"
          ]
        }
      ],
      "source": [
        "def predict(text):\n",
        "  with torch.no_grad():\n",
        "    input_ids = tokenizer.encode(text).ids\n",
        "    text = torch.tensor(input_ids).to(device)\n",
        "    output = model(text, torch.tensor([0]).to(device))\n",
        "    return output.argmax(1).item()\n",
        "\n",
        "ex_text_str = \"\"\"\n",
        "Despite the bustle of Christmas shoppers in Leeds, small businesses\n",
        "are sharing their concerns about the upcoming challenges they face.\n",
        "In Labour's first budget in 14 years,\n",
        "chancellor Rachel Reeves raised employer National Insurance contributions\n",
        "and announced minimum wage increases.\n",
        "\"\"\"\n",
        "\n",
        "print(\"This is a {} news\".format(ag_news_label[predict(ex_text_str)]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAZgBqbGcPb9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}