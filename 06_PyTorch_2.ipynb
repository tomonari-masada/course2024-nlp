{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNfBnQDCMRVNEMh07iz7UsH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/course2024-nlp/blob/main/06_PyTorch_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HumXxZix0r0F"
      },
      "source": [
        "# PyTorch入門（2）\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rt6hJJOC00hz"
      },
      "source": [
        "## 線形回帰モデル\n",
        "* 線形回帰モデルのmini-batch gradient descentをPyTorchで書いてみる。\n",
        "* PyTorchのDatasetとDataLoaderの使い方も併せて学ぶ。\n",
        "* ついでにwandbも使ってみます。"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 準備"
      ],
      "metadata": {
        "id": "mGXHmDqueTIk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ランタイムのタイプは、今回はCPUで構わないです。"
      ],
      "metadata": {
        "id": "Zn8GjzENcMqr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YX7vFs7IJh5K"
      },
      "source": [
        "* reproducibilityについては下記リンク先を参照\n",
        "  * https://pytorch.org/docs/stable/notes/randomness.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ki1IHrFY1B0J"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import random_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "%config InlineBackend.figure_format='retina'\n",
        "\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkVV10QS04Z6"
      },
      "source": [
        "## synthetic data\n",
        "* $y = w_1 x_1 + w_2 x_2 + b + \\epsilon$という式にしたがってデータを生成する。\n",
        "  * $\\epsilon$は正規分布に従うとする。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9CNqcC50zUu"
      },
      "source": [
        "# データ数\n",
        "data_size = 1000\n",
        "\n",
        "# ランダムな二次元ベクトルの集合を訓練データとして設定\n",
        "X = 10 * torch.rand(data_size, 2) - 5.0\n",
        "\n",
        "# 係数と切片の正解（これに近い値が求まればよい）\n",
        "w_true = torch.tensor([[2.0], [-3.0]])\n",
        "b_true = torch.tensor([10.0])\n",
        "\n",
        "# 正規乱数を加えた値がターゲット\n",
        "y = X @ w_true + b_true + torch.normal(0.0, 2.0, (data_size, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqCxr9NHO8yN"
      },
      "source": [
        "## Datasetクラス\n",
        "* `torch.utils.data.Dataset`を継承して自分用のデータセットのクラスを定義する。\n",
        "* 以下の２つの関数を必ず書く。\n",
        "  * データセットの長さを返す関数`__len__`\n",
        "  * 与えられたインデックスに対応するアイテムを返す関数`__getitem__`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_tNcrjsIS4y"
      },
      "source": [
        "class MyDataset(Dataset):\n",
        "  def __init__(self, X, y):\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.X.shape[0]\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.X[index], self.y[index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEklKEEfVGIi"
      },
      "source": [
        "### データセットの分割\n",
        "* ここでは train : valid : test = 8 : 1 : 1 に分割することにする。\n",
        "  * この分割の比率に、深い意味はない。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hW_iAHjHVCW8"
      },
      "source": [
        "dataset = MyDataset(X, y)\n",
        "train, val, test = random_split(dataset, [0.8, 0.1, 0.1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqEMzr7cKoc6"
      },
      "source": [
        "print(f\"training size: {len(train)}, validation size: {len(val)}, test size:{len(test)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 分割した後は、`torch.utils.data.Dataset`ではなく、`torch.utils.data.dataset.Subset`になる。"
      ],
      "metadata": {
        "id": "bPv-KYazlNFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "type(train)"
      ],
      "metadata": {
        "id": "zyyGnb2LlTsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train[:5]"
      ],
      "metadata": {
        "id": "OC8UD7OrWTdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9I7YBl2DPDD_"
      },
      "source": [
        "### 訓練データを可視化"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trYloF231kRn"
      },
      "source": [
        "plt.figure(figsize=(9, 4))\n",
        "\n",
        "X_train = train[:][0]\n",
        "y_train = train[:][1]\n",
        "\n",
        "ax1 = plt.subplot(121)\n",
        "ax1.scatter(X_train[:,0].numpy(), y_train.numpy(), c=\"b\", marker=\".\")\n",
        "plt.xlabel(\"x1\")\n",
        "plt.ylabel(\"y\", rotation=0)\n",
        "\n",
        "ax2 = plt.subplot(122)\n",
        "ax2.scatter(X_train[:,1].numpy(), y_train.numpy(), c=\"g\", marker=\".\")\n",
        "plt.xlabel(\"x2\")\n",
        "plt.ylabel(\"y\", rotation=0);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pm7eB5jCPLk3"
      },
      "source": [
        "## DataLoaderクラス\n",
        "* 訓練データをシャッフルしてミニバッチをひとつずつ取り出す処理を、PyTorchのDataLoaderを使って実装する。\n",
        "  * https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jq12Qs8KMXqQ"
      },
      "source": [
        "batch_size = 10\n",
        "\n",
        "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val, batch_size=batch_size)\n",
        "test_loader = DataLoader(test, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* データローダの長さは、ミニバッチの個数。インスタンスの個数ではない。"
      ],
      "metadata": {
        "id": "6BYdmNKQnFmE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_loader)"
      ],
      "metadata": {
        "id": "d1B3omXHnBcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 訓練データの最初のミニバッチだけ見てみる。"
      ],
      "metadata": {
        "id": "V4ZIJID4nM5B"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svJOzzlmlNkS"
      },
      "source": [
        "print(next(iter(train_loader)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QObgXLk26yQx"
      },
      "source": [
        "## モデルの定義と初期化\n",
        "* 値を推定したいのは、線形モデル$y = w_1 x_1 + w_2 x_2 + b$の係数$w_1,w_2$と切片$b$。\n",
        "* そこで、係数と切片を微分可能なテンソルとして用意する。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEwTHRWz6Lym"
      },
      "source": [
        "w = torch.randn((2, 1), requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erBDyaw_KsBi"
      },
      "source": [
        "print(w)\n",
        "print(b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2mw1jJF6aEF"
      },
      "source": [
        "## 損失関数\n",
        "* 平均二乗誤差を使う。\n",
        "* PyTorchで用意されている損失関数については、下のリンク先を参照。\n",
        " * https://pytorch.org/docs/stable/nn.html#loss-functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Azz3iDWU6X9P"
      },
      "source": [
        "criterion = torch.nn.MSELoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEgaQkHQ6e8B"
      },
      "source": [
        "## 最適化アルゴリズム\n",
        "* 今回は、SGDを使う。\n",
        " * 説明のため、あえて`momentum`は使わない。\n",
        "* PyTorchで用意されているoptimizerについては、下のリンク先を参照。\n",
        " * https://pytorch.org/docs/stable/optim.html#algorithms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULsUK_jV6dGS"
      },
      "source": [
        "optimizer = torch.optim.SGD([w, b], lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uhp2A3pkWaFn"
      },
      "source": [
        "## 評価用のヘルパ関数"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 評価する際には、計算グラフを作る必要はない。\n",
        "* `with torch.no_grad()`で計算グラフを作らないようにする。"
      ],
      "metadata": {
        "id": "_UBvqc-ODI3F"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mq0pGq3ftPQo"
      },
      "source": [
        "def evaluate(loader, w, b):\n",
        "  total_size = 0\n",
        "  total_loss = 0.0\n",
        "  for input, target in loader:\n",
        "    with torch.no_grad():\n",
        "      output = input @ w + b\n",
        "      loss = criterion(output, target)\n",
        "      total_size += len(target)\n",
        "      total_loss += loss.item() * len(target)\n",
        "  return total_loss, total_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxtW5Hu-bPPV"
      },
      "source": [
        "## 学習\n",
        "* ループの内側には、以下の４つを書く\n",
        "1. 損失関数の値を計算することによって、計算グラフを作る\n",
        "2. backpropagationの実行\n",
        "3. パラメータの更新\n",
        "4. gradientをゼロにする\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* コメントアウトしたprint関数を実行させると・・・\n",
        " * 本当に勾配を使ってwを更新していることが分かる。"
      ],
      "metadata": {
        "id": "_oVwjlqjlEk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epoch = 0\n",
        "step = 0"
      ],
      "metadata": {
        "id": "OoD--LhBewYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gm6sreTk3zZt"
      },
      "source": [
        "for _ in range(10):\n",
        "  epoch += 1\n",
        "\n",
        "  train_size = 0\n",
        "  train_loss = 0.0\n",
        "  for input, target in train_loader:\n",
        "    step += 1\n",
        "\n",
        "    output = input @ w + b\n",
        "    loss = criterion(output, target)\n",
        "    train_size += len(target)\n",
        "    train_loss += loss.item() * len(target) # ロギング用の集計\n",
        "\n",
        "    #print(f\"\\t step {step}: w before update {list(w.detach().numpy().flatten())}\")\n",
        "    loss.backward()\n",
        "    #print(f\"\\t\\t w.grad {list(w.grad.detach().numpy().flatten())}\")\n",
        "    optimizer.step()\n",
        "    #print(f\"\\t step {step}: w after update {list(w.detach().numpy().flatten())}\")\n",
        "    #print(\"-\"*60)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  # validation lossの計算\n",
        "  val_loss, val_size = evaluate(val_loader, w, b)\n",
        "\n",
        "  # print関数でロギング\n",
        "  print(f'epoch {epoch}: ',\n",
        "        f'train loss {train_loss/train_size:.4f} ',\n",
        "        f'validation loss {val_loss/val_size:.4f} ',\n",
        "        f'w={list(w.detach().numpy().flatten())}',\n",
        "        f'b={b.item():.4f}')\n",
        "  print(\"=\"*90)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Gps-Bc9wPDA"
      },
      "source": [
        "test_loss, test_size = evaluate(test_loader, w, b)\n",
        "print(f\"test loss {test_loss / test_size:8.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## wandb (Weights&Biases) を使う"
      ],
      "metadata": {
        "id": "WJNEGKy-LYQ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* https://www.wandb.jp/"
      ],
      "metadata": {
        "id": "hxPiZ9iafL12"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 参考資料\n",
        "  * 本家のquickstart https://docs.wandb.ai/quickstart\n",
        "  * 解説記事の例 https://zenn.dev/zenizeni/books/a64578f98450c2/viewer/e18fdf"
      ],
      "metadata": {
        "id": "IcZBflGmGiNO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### インストール"
      ],
      "metadata": {
        "id": "13CMUfS1Lcoa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "id": "yOpoBo8WGvNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ログイン"
      ],
      "metadata": {
        "id": "CzofzePjLefH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* APIキーを取得する。\n",
        "  * https://wandb.ai/authorize"
      ],
      "metadata": {
        "id": "JT33ZmRbGzrP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "id": "DR-9yfINGg8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### wandbの初期化"
      ],
      "metadata": {
        "id": "jF-fvRt9LqHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10\n",
        "learning_rate = 0.001\n",
        "epochs = 100\n",
        "\n",
        "run = wandb.init(\n",
        "    # プロジェクト名\n",
        "    project=\"my-linear-regression-project\",\n",
        "    # トラックするハイパーパラメータや実行のメタデータ\n",
        "    config={\n",
        "        \"batch_size\": batch_size,\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"epochs\": epochs,\n",
        "    },\n",
        ")"
      ],
      "metadata": {
        "id": "eilci9xpG_fz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 学習の準備\n",
        "* wandbには関係なく、上で書いたことをもう一度まとめて書いているだけ。"
      ],
      "metadata": {
        "id": "z_rpgWqoLtdN"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFvf0dEPQz-7"
      },
      "source": [
        "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val, batch_size=batch_size)\n",
        "test_loader = DataLoader(test, batch_size=batch_size)\n",
        "\n",
        "w = torch.randn((2, 1), requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.SGD([w, b], lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 学習\n",
        "* wandbでログをとっている。"
      ],
      "metadata": {
        "id": "nIlPgZ0vL2vy"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_1mkEnLWk7x"
      },
      "source": [
        "for epoch in range(epochs):\n",
        "\n",
        "  train_size = 0\n",
        "  train_loss = 0.0\n",
        "  for input, target in train_loader:\n",
        "    output = input @ w + b\n",
        "    loss = criterion(output, target)\n",
        "    train_size += len(target)\n",
        "    train_loss += loss.item() * len(target)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  wandb.log({\"loss/training\": train_loss / train_size})\n",
        "  val_loss, val_size = evaluate(val_loader, w, b)\n",
        "  wandb.log({\"loss/validation\": val_loss / val_size})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 結果を見に行く\n",
        "* https://wandb.ai/home"
      ],
      "metadata": {
        "id": "41LbPppgglfX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UngJV09JR8dF"
      },
      "source": [
        "## nn.Sequentialクラス\n",
        "* requires_grad=Trueでテンソルを作ればモデルを用意することはできる。\n",
        "* しかし、同じことは、torch.nnを使えばもっとすっきり実現できる。\n",
        "* まず、nn.Sequentialを使う方法を示す。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufK1j4NUQuzg"
      },
      "source": [
        "### nn.Sequentialのインスタンスとしてモデルを作る\n",
        "* 下記のようにモデルを作った時点でレイヤのパラメータは初期化されている。\n",
        "* この初期化には上でセットした乱数のシードが使われている。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMrXPs8j7AvJ"
      },
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Linear(2, 1),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiNUUuRpak-C"
      },
      "source": [
        "type(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upR9Es1yNFNZ"
      },
      "source": [
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* パラメータがどのように初期化されているかを確認してみる"
      ],
      "metadata": {
        "id": "vhPoNOwgkBvO"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_7hvKfH7PO2"
      },
      "source": [
        "for name, p in model.named_parameters():\n",
        "  print(name, p.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ak-6vuxtNPGr"
      },
      "source": [
        "### 学習のループ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3qhwI60gENN"
      },
      "source": [
        "batch_size = 10\n",
        "learning_rate = 0.001\n",
        "epochs = 100\n",
        "\n",
        "run = wandb.init(\n",
        "    # プロジェクト名\n",
        "    project=\"my-linear-regression-project\",\n",
        "    # トラックするハイパーパラメータや実行のメタデータ\n",
        "    config={\n",
        "        \"batch_size\": batch_size,\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"epochs\": epochs,\n",
        "    },\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val, batch_size=batch_size)\n",
        "test_loader = DataLoader(test, batch_size=batch_size)\n",
        "\n",
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "4XXpWxOQhkmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(loader, model):\n",
        "  total_size = 0\n",
        "  total_loss = 0.0\n",
        "  for input, target in loader:\n",
        "    with torch.no_grad():\n",
        "      output = model(input)\n",
        "      loss = criterion(output, target)\n",
        "      total_size += len(target)\n",
        "      total_loss += loss.item() * len(target)\n",
        "  return total_loss, total_size"
      ],
      "metadata": {
        "id": "CfqEEXxSk9e5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HCE8FD07o_w"
      },
      "source": [
        "for epoch in range(epochs):\n",
        "\n",
        "  train_size = 0\n",
        "  train_loss = 0.0\n",
        "  for input, target in train_loader:\n",
        "    output = model(input)\n",
        "    loss = criterion(output, target)\n",
        "    train_size += len(target)\n",
        "    train_loss += loss.item() * len(target)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  wandb.log({\"loss/training\": train_loss / train_size})\n",
        "  val_loss, val_size = evaluate(val_loader, model)\n",
        "  wandb.log({\"loss/validation\": val_loss / val_size})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FO43yHMyapoA"
      },
      "source": [
        "## nn.Moduleクラス\n",
        "* nn.Moduleを継承するクラスを定義する。\n",
        "* そしてそのクラスのインスタンスとしてモデルを作る。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyLinearModel(nn.Module):\n",
        "  def __init__(self, input_size, output_size):\n",
        "    super().__init__()\n",
        "    self.fc = nn.Linear(input_size, output_size)\n",
        "\n",
        "  def forward(self, input):\n",
        "    return self.fc(input)"
      ],
      "metadata": {
        "id": "TyD69Xs9m6K0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYYTbktFb9zV"
      },
      "source": [
        "model = MyLinearModel(2, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YI3UMZc7d7NS"
      },
      "source": [
        "for name, p in model.named_parameters():\n",
        "  print(name, p.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "303BtfTqd8aC"
      },
      "source": [
        "batch_size = 10\n",
        "learning_rate = 0.001\n",
        "epochs = 100\n",
        "\n",
        "run = wandb.init(\n",
        "    # プロジェクト名\n",
        "    project=\"my-linear-regression-project\",\n",
        "    # トラックするハイパーパラメータや実行のメタデータ\n",
        "    config={\n",
        "        \"batch_size\": batch_size,\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"epochs\": epochs,\n",
        "    },\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s71a9ARngVCf"
      },
      "source": [
        "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val, batch_size=batch_size)\n",
        "test_loader = DataLoader(test, batch_size=batch_size)\n",
        "\n",
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTdfKC0BdtmC"
      },
      "source": [
        "for epoch in range(epochs):\n",
        "\n",
        "  train_size = 0\n",
        "  train_loss = 0.0\n",
        "  for input, target in train_loader:\n",
        "    output = model(input)\n",
        "    loss = criterion(output, target)\n",
        "    train_size += len(target)\n",
        "    train_loss += loss.item() * len(target)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  wandb.log({\"loss/training\": train_loss / train_size})\n",
        "  val_loss, val_size = evaluate(val_loader, model)\n",
        "  wandb.log({\"loss/validation\": val_loss / val_size})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 予告\n",
        "* 次の回では、下のPyTorchのチュートリアルを、ほぼそのまま使います。\n",
        " * https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html"
      ],
      "metadata": {
        "id": "kUMeS1lBG5Y8"
      }
    }
  ]
}