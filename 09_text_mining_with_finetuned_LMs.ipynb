{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/course2024-nlp/blob/main/09_text_mining_with_finetuned_LMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# テキストマイニングのためのLMのfinetuning"
      ],
      "metadata": {
        "id": "JXkV5OKbCXib"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* たぶん、このnotebookの説明は、授業一回では終わりません・・・。"
      ],
      "metadata": {
        "id": "_mWyhegzUcM7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 技術的な背景\n",
        "* 今ではBERT largeの規模の言語モデルでも気軽に使える。\n",
        "* 今までならトピックモデルを使っていただろうトピック抽出も気軽に行える。\n",
        "* さらに、今ではBERT large規模の言語モデルのファインチューニングも気軽に実行できる。\n",
        "* ということは、分析対象のテキスト集合を使って言語モデルウをファインチューニングすれば・・・\n",
        "* 分析対象のテキスト集合向けにカスタマイズされたトピック抽出も、気軽にできるようになっている。"
      ],
      "metadata": {
        "id": "HzBVRWNUCfQY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## インストール"
      ],
      "metadata": {
        "id": "0cTvxMiBDPIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers datasets sentence-transformers captum"
      ],
      "metadata": {
        "id": "H5nSnAXZDViY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## インポート"
      ],
      "metadata": {
        "id": "MMp10EqXDmfu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LtP5EwVUidp"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.functional import normalize\n",
        "\n",
        "from datasets import Dataset, DatasetDict, load_dataset\n",
        "from transformers import set_seed\n",
        "from sentence_transformers import (\n",
        "    SentenceTransformer,\n",
        "    SentenceTransformerTrainer,\n",
        "    SentenceTransformerTrainingArguments,\n",
        ")\n",
        "from sentence_transformers.losses import TripletLoss\n",
        "from sentence_transformers.evaluation import TripletEvaluator\n",
        "\n",
        "set_seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 言語モデルを使ったトピック抽出"
      ],
      "metadata": {
        "id": "odXlFeRQIcMT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### データセット\n",
        "* ライブドアニュースコーパスからトピックを抽出する。\n",
        "  * 分類カテゴリは今回は使わない。"
      ],
      "metadata": {
        "id": "UER2uElEDoLz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbsbkCo8Uidq"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\n",
        "    \"shunk031/livedoor-news-corpus\",\n",
        "    train_ratio=0.8, val_ratio=0.1, test_ratio=0.1,\n",
        "    random_state=42,\n",
        "    shuffle=True,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "num_categories = len(set(dataset[\"train\"][\"category\"]))\n",
        "max_seq_length = 512\n",
        "\n",
        "category_names = ['movie-enter', 'it-life-hack', 'kaden-channel', 'topic-news', 'livedoor-homme', 'peachy', 'sports-watch', 'dokujo-tsushin', 'smax']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHKbPWfzUidq"
      },
      "outputs": [],
      "source": [
        "dataset[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 言語モデル\n",
        "* 今回は`intfloat/multilingual-e5-large-instruct`を使う。\n",
        "  * 日本語にも対応しているため。\n",
        "* sentence transformersというライブラリを使ってモデルを操作する。\n",
        "  * Hugging Faceのtransformersよりも初心者に優しい。"
      ],
      "metadata": {
        "id": "ZglqQMM3ECeM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"intfloat/multilingual-e5-large-instruct\"\n",
        "model = SentenceTransformer(model_id)"
      ],
      "metadata": {
        "id": "LO63XJgWEBvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "ur4mFAEfF2lP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* sentence transformersではデフォルトでモデルがGPU上に映される。"
      ],
      "metadata": {
        "id": "sAgvM053F-tM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.device"
      ],
      "metadata": {
        "id": "QlgxE8zNF4cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* モデルのパラメータ一覧"
      ],
      "metadata": {
        "id": "teRm2noIGdUZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for name, _ in model.named_parameters():\n",
        "  print(name)"
      ],
      "metadata": {
        "id": "zv62lmzFGJ3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### テキストの埋め込み"
      ],
      "metadata": {
        "id": "yTRLLC0PFvr2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 今回は（時間節約のため）タイトルを使う。"
      ],
      "metadata": {
        "id": "M2VbZripGq19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = {}\n",
        "for key in dataset:\n",
        "  embeddings[key] = model.encode(\n",
        "      dataset[key][\"title\"],\n",
        "      show_progress_bar=True,\n",
        "  )"
      ],
      "metadata": {
        "id": "Ri-U0GIYFxwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### テキストのクラスタリング"
      ],
      "metadata": {
        "id": "ktVumaChG152"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* クラスタ数は適当に設定する。"
      ],
      "metadata": {
        "id": "IhiWSmTaG4uV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_clusters = 30\n",
        "kmeans = KMeans(n_clusters=n_clusters, n_init='auto', random_state=0)\n",
        "kmeans.fit(embeddings[\"train\"])\n",
        "centers = kmeans.cluster_centers_"
      ],
      "metadata": {
        "id": "-cvd5_tzG8dq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* クラスタの重心に近い順に数件のテキストを表示させる。"
      ],
      "metadata": {
        "id": "eWxpDqc9IRON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "similarities = cosine_similarity(embeddings[\"train\"], centers)\n",
        "for i in range(similarities.shape[-1]):\n",
        "  indices = np.argsort(- similarities[:,i])[:5]\n",
        "  for index in indices:\n",
        "    print(f\"{i:d} {index:d} \" + dataset[\"train\"][\"title\"][index])"
      ],
      "metadata": {
        "id": "0B4vkKygHVyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### クラスタのラベリング"
      ],
      "metadata": {
        "id": "0CAx0N_JIpXQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 下に示すのは、あくまで一つの方法。"
      ],
      "metadata": {
        "id": "QpPGg79oIrrc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 日本語テキストを形態素解析できるようにする。\n",
        "  * セッションの再起動はおそらく不要。\n"
      ],
      "metadata": {
        "id": "eH9EFyaDI7tc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download ja_core_news_sm"
      ],
      "metadata": {
        "id": "X0tGh0WDJR2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 形態素解析の実行。"
      ],
      "metadata": {
        "id": "-wNNDBSeJhyx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_pos_tags = [\"NOUN\", \"PROPN\"]\n",
        "\n",
        "nlp = spacy.load(\"ja_core_news_sm\")\n",
        "corpus = {}\n",
        "for key in dataset:\n",
        "  corpus[key] = []\n",
        "  for text in tqdm(dataset[key][\"title\"]):\n",
        "    corpus[key].append(\" \".join(\n",
        "        [token.lemma_\n",
        "         for token in nlp(text) if token.pos_ in label_pos_tags\n",
        "        ]\n",
        "    ))"
      ],
      "metadata": {
        "id": "yeLWry5rIwvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* scikit-learnのTfidfVectorizerを使う。\n",
        "  * min_dfやmax_dfの設定は、クラスタのラベリングに使う単語を絞り込む。"
      ],
      "metadata": {
        "id": "p3dOiUblJq1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(min_df=10, max_df=0.1, lowercase=False)\n",
        "vectorizer.fit(corpus[\"train\"])\n",
        "vocab = np.array(vectorizer.get_feature_names_out())\n",
        "X_train = vectorizer.transform(corpus[\"train\"]).toarray()"
      ],
      "metadata": {
        "id": "e9wP3zdOJlWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab"
      ],
      "metadata": {
        "id": "3TEQSPkoJ69M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 単語を含むテキストのtf-idf値を使って、テキストの重みづけ線形和を求める。\n",
        "* 得られた線形和をその単語の埋め込みとみなす。"
      ],
      "metadata": {
        "id": "Md1Kq5U4KMkr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_embeddings = np.dot((X_train / X_train.sum(0)).T, embeddings[\"train\"])"
      ],
      "metadata": {
        "id": "Mt3SVsf0KF6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* テキストのクラスタの重心に近い順に20個の単語を表示させる。"
      ],
      "metadata": {
        "id": "t0E2g5HFKo4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic_words = []\n",
        "similarities = cosine_similarity(vocab_embeddings, centers)\n",
        "for i in range(similarities.shape[-1]):\n",
        "  indices = np.argsort(- similarities[:,i])\n",
        "  topic_words.append(f\"{i:d} \" + \" \".join(list(vocab[indices[:20]])))\n",
        "print(\"\\n\".join(topic_words))"
      ],
      "metadata": {
        "id": "isTs8cJVKWtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 言語モデルのファインチューニング"
      ],
      "metadata": {
        "id": "Chw13fl9K1h5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### トリプレットデータを作成する\n",
        "* anchor, positive, negativeという3つのテキストの組。\n",
        "  * positiveはanchorと同じクラスに属するテキスト。\n",
        "  * negativeはanchorと異なるクラスに属するテキスト\n",
        "* この三つ組のテキストを、後でファインチューニングに使う。"
      ],
      "metadata": {
        "id": "JwZzekqoLNsh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lO8FJFW9Uidr"
      },
      "outputs": [],
      "source": [
        "triplet_dataset = {}\n",
        "for key in dataset:\n",
        "\n",
        "  categorized = [list() for i in range(num_categories)]\n",
        "  for example in dataset[key]:\n",
        "    categorized[example[\"category\"]].append(example[\"title\"])\n",
        "  category_size = [len(categorized[i]) for i in range(num_categories)]\n",
        "\n",
        "  anchors, positives, negatives = [], [], []\n",
        "  for i in range(num_categories):\n",
        "    indices = i + np.random.randint(1, num_categories, category_size[i])\n",
        "    indices = indices % num_categories\n",
        "    anchors += categorized[i]\n",
        "    positives += [\n",
        "        categorized[i][np.random.randint(0, category_size[i])]\n",
        "        for _ in indices\n",
        "    ]\n",
        "    negatives += [\n",
        "        categorized[j][np.random.randint(0, category_size[j])]\n",
        "        for j in indices\n",
        "    ]\n",
        "\n",
        "  triplet_dataset[key] = Dataset.from_dict({\n",
        "      \"anchors\": anchors,\n",
        "      \"positives\": positives,\n",
        "      \"negatives\": negatives,\n",
        "  })\n",
        "\n",
        "triplet_dataset = DatasetDict(triplet_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZZD3tgKUidr"
      },
      "outputs": [],
      "source": [
        "train_dataset = triplet_dataset[\"train\"]\n",
        "eval_dataset = triplet_dataset[\"validation\"]\n",
        "test_dataset = triplet_dataset[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLCE-XSzUidr"
      },
      "outputs": [],
      "source": [
        "train_dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 損失関数"
      ],
      "metadata": {
        "id": "IR0irbvxL8C-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* トリプレット損失関数を使う。\n",
        "  * anchorとpositiveの埋め込みベクトルは接近し・・・\n",
        "  * anchorとnegativeの埋め込みベクトルは遠ざかるように・・・\n",
        "  * ファインチューニングが進む。"
      ],
      "metadata": {
        "id": "2jY5mAKgL9U7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgF3PttJUidr"
      },
      "outputs": [],
      "source": [
        "loss = TripletLoss(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### trainerの設定"
      ],
      "metadata": {
        "id": "puPAp8BcMMgr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9aNYqGxUidr"
      },
      "outputs": [],
      "source": [
        "args = SentenceTransformerTrainingArguments(\n",
        "    output_dir=f\"models/{model_id}_livedoor-title-triplet\",\n",
        "    max_steps=1000,\n",
        "    learning_rate=1e-4,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=8,\n",
        "    bf16=True,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=100,\n",
        "    save_total_limit=2,\n",
        "    logging_steps=100,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### evaluatorの設定"
      ],
      "metadata": {
        "id": "Q5pvmb5lMVNt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-cAXd9mUids"
      },
      "outputs": [],
      "source": [
        "dev_evaluator = TripletEvaluator(\n",
        "    anchors=eval_dataset[\"anchors\"],\n",
        "    positives=eval_dataset[\"positives\"],\n",
        "    negatives=eval_dataset[\"negatives\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ファインチューニングの実行前に、evaluatorに評価させてみる。"
      ],
      "metadata": {
        "id": "NP-fSCjDMmVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dev_evaluator(model)"
      ],
      "metadata": {
        "id": "7wK1C6qPMmC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### trainerの作成"
      ],
      "metadata": {
        "id": "5fwV3_uHMdXz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItVSVgaZUids"
      },
      "outputs": [],
      "source": [
        "trainer = SentenceTransformerTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    loss=loss,\n",
        "    evaluator=dev_evaluator,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* trainableなパラメータの個数を確認してみる。\n",
        "  * 100%になっているはず。"
      ],
      "metadata": {
        "id": "TxHg152aNPvu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KdIeApb8Uids"
      },
      "outputs": [],
      "source": [
        "def print_trainable_parameters(model):\n",
        "  trainable_params = 0\n",
        "  all_param = 0\n",
        "  for _, param in model.named_parameters():\n",
        "    all_param += param.numel()\n",
        "    if param.requires_grad:\n",
        "      trainable_params += param.numel()\n",
        "  print(\n",
        "      f\"trainable params: {trainable_params} \"\n",
        "      f\"|| all params: {all_param} \"\n",
        "      f\"|| trainable%: {100 * trainable_params / all_param}\"\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSfAGxlfUids"
      },
      "outputs": [],
      "source": [
        "print_trainable_parameters(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ファインチューニングの実行"
      ],
      "metadata": {
        "id": "Jp-AvfrGMwt0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TpeROacUids"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ファインチューニング後のモデルによるテキストの埋め込み"
      ],
      "metadata": {
        "id": "Gsz3pPJrNn2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = {}\n",
        "for key in dataset:\n",
        "  embeddings[key] = model.encode(\n",
        "      dataset[key][\"title\"],\n",
        "      show_progress_bar=True,\n",
        "  )"
      ],
      "metadata": {
        "id": "sThP5RC0NmzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 語彙の埋め込み"
      ],
      "metadata": {
        "id": "n9t3pWUdOcAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_embeddings = np.dot((X_train / X_train.sum(0)).T, embeddings[\"train\"])"
      ],
      "metadata": {
        "id": "kfW2N631OblW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### テキストのクラスタリング"
      ],
      "metadata": {
        "id": "xjBN44H7NvOB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxaFTGfSUidt"
      },
      "outputs": [],
      "source": [
        "n_clusters = 30\n",
        "kmeans = KMeans(n_clusters=n_clusters, n_init='auto', random_state=123)\n",
        "kmeans.fit(embeddings[\"train\"])\n",
        "centers = kmeans.cluster_centers_"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### クラスタのラベリング"
      ],
      "metadata": {
        "id": "6rryw4MLOijL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfZRDAfGUidu"
      },
      "outputs": [],
      "source": [
        "topic_words = []\n",
        "similarities = cosine_similarity(vocab_embeddings, centers)\n",
        "for i in range(similarities.shape[-1]):\n",
        "    indices = np.argsort(- similarities[:,i])\n",
        "    topic_words.append(f\"{i:d} \" + \" \".join(list(vocab[indices[:20]])))\n",
        "print(\"\\n\".join(topic_words))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 言語モデルの解釈\n",
        "* Integrated Gradientsという手法を使う。\n",
        "  * Captumというライブラリに実装がある。\n",
        "* 特定のクラスタにテキストが所属する\"根拠\"を調査できる。\n",
        "  * どのトークンがその所属に効いているかを可視化できる。"
      ],
      "metadata": {
        "id": "RDuXQfrsOtQj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* モデルのパラメータを微分できない状態にする。\n",
        "  * メモリを節約するため。"
      ],
      "metadata": {
        "id": "_-ZbtLBXPBs7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G98FcKmrUidu"
      },
      "outputs": [],
      "source": [
        "for param in model.parameters():\n",
        "  param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* モデルをCPUに移す。\n",
        "  * 無料版のGoogle ColabではGPUのメモリが足りなくなるため。"
      ],
      "metadata": {
        "id": "SBlXG5jMPN5K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKHQUqPeUidu"
      },
      "outputs": [],
      "source": [
        "model.to(\"cpu\").eval();"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 準備"
      ],
      "metadata": {
        "id": "0HNF6Z9APUj9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7R4Ln9TzUidu"
      },
      "outputs": [],
      "source": [
        "from captum.attr import LayerIntegratedGradients, TokenReferenceBase, visualization\n",
        "\n",
        "tokenizer = model.tokenizer\n",
        "token_reference = TokenReferenceBase(reference_token_idx=tokenizer.pad_token_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Captum向けの埋め込みの書き方"
      ],
      "metadata": {
        "id": "oRGzPrmtSGkj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* input_idsとattention_maskを使った埋め込みに書き換える。\n",
        "  * Captumではsentence transformersにおける言語モデルのencodeメソッドは使えない。"
      ],
      "metadata": {
        "id": "ekDsk0JESLuV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbIXF1yDUidu"
      },
      "outputs": [],
      "source": [
        "text = dataset[\"train\"][\"title\"][0]\n",
        "encodings = tokenizer(text, padding=True, return_tensors=\"pt\")\n",
        "encodings = encodings.to(model.device)\n",
        "input_ids = encodings[\"input_ids\"]\n",
        "attention_mask = encodings[\"attention_mask\"]\n",
        "with torch.no_grad():\n",
        "  embedding = model({\"input_ids\": input_ids, \"attention_mask\": attention_mask})[\"sentence_embedding\"]\n",
        "  st_embedding = model.encode(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* encodeメソッドと埋め込みベクトルが一致することを確認する。"
      ],
      "metadata": {
        "id": "gj-UJ7L1SnHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "((embedding - st_embedding) ** 2).sum()"
      ],
      "metadata": {
        "id": "tdrliAu2RseC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### テキストとクラスタ重心との類似度を求めるヘルパ関数"
      ],
      "metadata": {
        "id": "rbLP3x5JRVJG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 類似度はコサイン類似度で求める。\n",
        "  * ベクトルの長さが1なので、大小関係はユークリッド距離と同じ。"
      ],
      "metadata": {
        "id": "_CwW7dNnS0W2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FuI7-JKyUidu"
      },
      "outputs": [],
      "source": [
        "cos_sim = nn.CosineSimilarity(dim=-1)\n",
        "cluster_centers = torch.tensor(centers, device=model.device)\n",
        "\n",
        "def predict(input_ids, attention_mask):\n",
        "  embedding = model({\n",
        "      \"input_ids\": input_ids,\n",
        "      \"attention_mask\": attention_mask,\n",
        "  })[\"sentence_embedding\"]\n",
        "  return cos_sim(\n",
        "      cluster_centers.unsqueeze(0),\n",
        "      embedding.unsqueeze(1)\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 試しに一つのテキストについて各クラスタとの類似度を計算してみる。"
      ],
      "metadata": {
        "id": "yLYpFGgrTAVQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rQvRyL9Uidu"
      },
      "outputs": [],
      "source": [
        "text = dataset[\"train\"][\"title\"][0]\n",
        "print(text)\n",
        "encodings = tokenizer(text, padding=True, return_tensors=\"pt\")\n",
        "encodings.to(model.device)\n",
        "predict(\n",
        "    encodings.input_ids,\n",
        "    encodings.attention_mask,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### テキストと特定のクラスタの重心との類似度を求めるヘルパ関数"
      ],
      "metadata": {
        "id": "in9co_mPTILT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYSgFSEnUidu"
      },
      "outputs": [],
      "source": [
        "def cluster_similarity_forward_func(input_ids, attention_mask, cluster_id):\n",
        "  similarities = predict(input_ids, attention_mask)\n",
        "  return similarities[:,cluster_id]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pov6TQv0Uidv"
      },
      "outputs": [],
      "source": [
        "text = dataset[\"train\"][\"title\"][0]\n",
        "encodings = tokenizer(text, padding=True, return_tensors=\"pt\")\n",
        "encodings.to(model.device)\n",
        "cluster_similarity_forward_func(\n",
        "    encodings.input_ids,\n",
        "    encodings.attention_mask,\n",
        "    29,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Integrated Gradientの初期化"
      ],
      "metadata": {
        "id": "c2E7ZLHdTSgz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* モデルのembedding layerを取得する必要がある。"
      ],
      "metadata": {
        "id": "aoFQ-WAKTVf6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9BJ246gUidv"
      },
      "outputs": [],
      "source": [
        "list(model[0].modules())[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ya_q8wDCUidv"
      },
      "outputs": [],
      "source": [
        "list(model[0].modules())[1].embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJzKbW0FUidv"
      },
      "outputs": [],
      "source": [
        "lig = LayerIntegratedGradients(\n",
        "    cluster_similarity_forward_func,\n",
        "    list(model[0].modules())[1].embeddings.word_embeddings,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 可視化のためのヘルパ関数"
      ],
      "metadata": {
        "id": "ukpWoH3QTaFY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 可視化結果を蓄えるリスト"
      ],
      "metadata": {
        "id": "8WJt7scTTwEK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H30SfWGYUidv"
      },
      "outputs": [],
      "source": [
        "vis_data_records_ig = []"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 可視化のためのヘルパ関数"
      ],
      "metadata": {
        "id": "j5ZsJXHzTyFF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8S8w7qfuUidv"
      },
      "outputs": [],
      "source": [
        "def add_attributions_to_visualizer(attributions, text, pred_prob, pred_class, true_class,\n",
        "                                   attr_class, convergence_scores, vis_data_records):\n",
        "  attributions = attributions.cpu()\n",
        "  attributions = attributions.sum(dim=-1).squeeze(0)\n",
        "  attributions = attributions / torch.norm(attributions)\n",
        "  attributions = attributions.cpu().detach().numpy()\n",
        "  vis_data_records.append(\n",
        "      visualization.VisualizationDataRecord(\n",
        "          attributions,\n",
        "          pred_prob,\n",
        "          pred_class,\n",
        "          true_class,\n",
        "          attr_class,\n",
        "          attributions.sum(),\n",
        "          text,\n",
        "          convergence_scores,\n",
        "      )\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 解釈を実行する関数"
      ],
      "metadata": {
        "id": "e5CkObAST0j6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6P3mf-VxUidv"
      },
      "outputs": [],
      "source": [
        "def interpret_text(text, attr_class=None, n_steps=50):\n",
        "  encodings = tokenizer(text, padding=True, return_tensors=\"pt\")\n",
        "  encodings = encodings.to(model.device)\n",
        "  input_ids = encodings.input_ids\n",
        "  attention_mask = encodings.attention_mask\n",
        "  tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
        "  reference_input_ids = token_reference.generate_reference(\n",
        "      len(tokens),\n",
        "      device=model.device,\n",
        "  ).unsqueeze(0)\n",
        "\n",
        "  similarities = predict(\n",
        "      input_ids,\n",
        "      attention_mask,\n",
        "  )\n",
        "  prediction = similarities.argmax().item()\n",
        "  if attr_class is None:\n",
        "    attr_class = prediction\n",
        "  print(\n",
        "      f\"prediction={prediction} \"\n",
        "      f\"cos_sim={similarities.max().item():.3f} \",\n",
        "      end=\"\"\n",
        "  )\n",
        "\n",
        "  attributions_ig, delta = lig.attribute(\n",
        "      input_ids,\n",
        "      reference_input_ids,\n",
        "      additional_forward_args=(attention_mask, attr_class),\n",
        "      n_steps=n_steps,\n",
        "      return_convergence_delta=True,\n",
        "  )\n",
        "  print(f\"convergence delta={delta.item():.3e} when n_steps={n_steps}\")\n",
        "\n",
        "  add_attributions_to_visualizer(\n",
        "      attributions_ig,\n",
        "      tokens,\n",
        "      similarities.max().item(),\n",
        "      str(prediction),\n",
        "      str(prediction),\n",
        "      str(attr_class),\n",
        "      delta,\n",
        "      vis_data_records_ig,\n",
        "  )\n",
        "  return prediction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Integrated Gradientsの実行"
      ],
      "metadata": {
        "id": "8he_C8UTT31y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vw8hJGlAUidv"
      },
      "outputs": [],
      "source": [
        "vis_data_records_ig = []\n",
        "for n_steps in [50, 100]:\n",
        "  interpret_text(dataset[\"train\"][\"title\"][0], n_steps=n_steps)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 解釈結果の可視化"
      ],
      "metadata": {
        "id": "oK8OdkQhT_FY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ステップ数によって結果が微妙に変わる"
      ],
      "metadata": {
        "id": "Lu81SaKOUBHn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVPh8cuQUidw"
      },
      "outputs": [],
      "source": [
        "visualization.visualize_text(vis_data_records_ig);"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 50件ぐらいのテキストのクラスタへの所属をまとめて解釈する。"
      ],
      "metadata": {
        "id": "P4R5R6MIUM8m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApanHAvcUidw"
      },
      "outputs": [],
      "source": [
        "for i in tqdm(range(50)):\n",
        "  example = dataset[\"validation\"][i]\n",
        "  print(category_names[example[\"category\"]], end=\" \")\n",
        "  vis_data_records_ig = []\n",
        "  prediction = interpret_text(example[\"title\"], n_steps=50)\n",
        "  print(\"\\t\" + topic_words[prediction])\n",
        "  visualization.visualize_text(vis_data_records_ig);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Hcp0xozUidw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}