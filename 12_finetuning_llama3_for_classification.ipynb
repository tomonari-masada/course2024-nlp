{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/course2024-nlp/blob/main/12_finetuning_llama3_for_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLMによるテキストの埋め込み\n"
      ],
      "metadata": {
        "id": "XmTED_q_Cf55"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## autoregressive LM for text embedding\n",
        "* パラメータ数が7~8BのLLMをテキスト分類に使うとき・・・\n",
        "* 普通はクラスラベルをテキストとして出力させる。\n",
        "  * autoregressive modelの普通の使い方は、やはりテキストの生成。\n",
        "* しかし、あえてLLMをテキストの埋め込みに使ってみる。\n",
        "  * つまり、BERTと同じような使い方をする。\n",
        "* また、埋め込みのためのモデルとしてのファインチューニングも行う。"
      ],
      "metadata": {
        "id": "31nj2jFoDSYV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## インストール"
      ],
      "metadata": {
        "id": "_bZ8lyypDgE4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpW2wxow1kMf"
      },
      "outputs": [],
      "source": [
        "!pip install -U transformers datasets bitsandbytes accelerate peft trl"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## インポート"
      ],
      "metadata": {
        "id": "BO4PBv6XD9v0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txIoIRcL1kMg"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    set_seed,\n",
        "    BitsAndBytesConfig,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "from transformers.models.llama.modeling_llama import (\n",
        "    LlamaForSequenceClassification,\n",
        ")\n",
        "from transformers.modeling_outputs import ModelOutput\n",
        "from peft import LoraConfig\n",
        "from trl import SFTTrainer\n",
        "\n",
        "set_seed(123)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGjG_J841kMh"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\n",
        "    \"shunk031/livedoor-news-corpus\",\n",
        "    train_ratio=0.8,\n",
        "    val_ratio=0.1,\n",
        "    test_ratio=0.1,\n",
        "    random_state=42,\n",
        "    shuffle=True,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "num_categories = len(set(dataset[\"train\"][\"category\"]))\n",
        "\n",
        "max_seq_length = 512"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 今回はtitleを使う。"
      ],
      "metadata": {
        "id": "MhM2MvDdEVLT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"train\"][\"title\"][:10]"
      ],
      "metadata": {
        "id": "CFktXolDENZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "category_names = [\n",
        "  'movie-enter',\n",
        "  'it-life-hack',\n",
        "  'kaden-channel',\n",
        "  'topic-news',\n",
        "  'livedoor-homme',\n",
        "  'peachy',\n",
        "  'sports-watch',\n",
        "  'dokujo-tsushin',\n",
        "  'smax',\n",
        "]"
      ],
      "metadata": {
        "id": "lAT-V5ZJEJuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 分類モデルの定義\n",
        "* テキストの末尾のトークンに対応する出力を分類に使う。\n",
        "* 今回使うLLMのクラスを継承して、新たなクラスを定義する。\n",
        "* 今回使うLLMは`LlamaForSequenceClassification`\n",
        "  * https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py"
      ],
      "metadata": {
        "id": "roBFvKuLB1ky"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXa-sR9_1kMh"
      },
      "outputs": [],
      "source": [
        "class LivedoorNet(LlamaForSequenceClassification):\n",
        "  def __init__(self, *args, **kwargs):\n",
        "    super(LivedoorNet, self).__init__(*args, **kwargs)\n",
        "\n",
        "  def forward(\n",
        "      self,\n",
        "      input_ids,\n",
        "      category=None,\n",
        "      attention_mask=None,\n",
        "      output_attentions=None,\n",
        "      output_hidden_states=None,\n",
        "      return_dict=None,\n",
        "      inputs_embeds=None,\n",
        "      labels=None,\n",
        "  ):\n",
        "    outputs = super(LivedoorNet, self).forward(\n",
        "        input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        output_attentions=output_attentions,\n",
        "        output_hidden_states=output_hidden_states,\n",
        "        return_dict=return_dict,\n",
        "    )\n",
        "    loss_fct = nn.CrossEntropyLoss()\n",
        "    loss = loss_fct(outputs.logits, category)\n",
        "    return ModelOutput(\n",
        "        loss=loss,\n",
        "        logits=outputs.logits,\n",
        "        past_key_values=outputs.past_key_values,\n",
        "        hidden_states=outputs.hidden_states,\n",
        "        attentions=outputs.attentions,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## モデルの取得\n",
        "* 今回は`tokyotech-llm/Llama-3-Swallow-8B-Instruct-v0.1`を使う。\n",
        "  * https://huggingface.co/tokyotech-llm/Llama-3-Swallow-8B-Instruct-v0.1\n",
        "* NF4量子化とDouble Quantizationの詳細は下の論文を参照。\n",
        "  * https://arxiv.org/abs/2305.14314\n",
        "* 量子化については下の記事も参考になる。\n",
        "  * https://huggingface.co/blog/4bit-transformers-bitsandbytes"
      ],
      "metadata": {
        "id": "kjO9c5zkEte8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHoHamRa1kMh"
      },
      "outputs": [],
      "source": [
        "model_name = \"tokyotech-llm/Llama-3-Swallow-8B-Instruct-v0.1\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_storage=torch.bfloat16,\n",
        ")\n",
        "\n",
        "model = LivedoorNet.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=num_categories,\n",
        "    quantization_config=bnb_config,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    low_cpu_mem_usage=True,\n",
        "    use_cache=False, # https://github.com/huggingface/transformers/issues/33489\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, max_seq_length=max_seq_length)\n",
        "\n",
        "# pad_tokenをeos_tokenに設定しないと、\n",
        "# 各トークン列の末尾のトークンではなく、\n",
        "# ミニバッチの中の最も長いトークン列の末尾で、\n",
        "# 分類用のlogitを取得してしまう。\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.config.pad_token_id = model.config.eos_token_id"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "5PNgF2yeKCTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LoRAの設定"
      ],
      "metadata": {
        "id": "V4N92sbLHpfy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nVfsZVd1kMh"
      },
      "outputs": [],
      "source": [
        "peft_config = LoraConfig(\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    r=32,\n",
        "    bias=\"none\",\n",
        "    task_type=\"SEQ_CLS\",\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## trainerの設定"
      ],
      "metadata": {
        "id": "k5SbZMQjHrTt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5S8AuCV1kMh"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    output_dir=\"outputs_cls\",\n",
        "    max_steps=1000,\n",
        "    eval_steps=100,\n",
        "    logging_steps=100,\n",
        "    save_steps=100,\n",
        "    learning_rate=5e-5,\n",
        "    eval_strategy=\"steps\",\n",
        "    logging_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    load_best_model_at_end=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## trainerの作成"
      ],
      "metadata": {
        "id": "hKdsf9gjHuP-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkDirGme1kMi"
      },
      "outputs": [],
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"validation\"],\n",
        "    dataset_text_field=\"title\",\n",
        "    peft_config=peft_config,\n",
        "    max_seq_length=max_seq_length,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 以下を実行しないと、エラーが出る。\n",
        "  * trainerを作成するとき、カテゴリの情報が消されてしまうため。"
      ],
      "metadata": {
        "id": "0MGzPp6kHxXc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NL7iUdNY1kMi"
      },
      "outputs": [],
      "source": [
        "trainer.train_dataset = trainer.train_dataset.add_column(\"category\", dataset[\"train\"][\"category\"])\n",
        "trainer.eval_dataset = trainer.eval_dataset.add_column(\"category\", dataset[\"validation\"][\"category\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## trainableなパラメータの確認"
      ],
      "metadata": {
        "id": "mvrRpapyJsk5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model, verbose=False):\n",
        "  trainable_params = 0\n",
        "  all_param = 0\n",
        "  for name, param in model.named_parameters():\n",
        "    all_param += param.numel()\n",
        "    if param.requires_grad:\n",
        "      trainable_params += param.numel()\n",
        "      if verbose: print(name)\n",
        "  print(\n",
        "      f\"trainable params: {trainable_params} \"\n",
        "      f\"|| all params: {all_param} \"\n",
        "      f\"|| trainable%: {100 * trainable_params / all_param}\"\n",
        "  )"
      ],
      "metadata": {
        "id": "CU0Z_NBAI7OB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_trainable_parameters(trainer.model)"
      ],
      "metadata": {
        "id": "4Nc39krxJqCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_trainable_parameters(trainer.model, verbose=True)"
      ],
      "metadata": {
        "id": "5LDDq_KkJ5RL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 正解率を計算するヘルパ関数"
      ],
      "metadata": {
        "id": "P-8oVt9rH7QQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgQ9jutG1kMi"
      },
      "outputs": [],
      "source": [
        "def evaluate_by_accuracy(model, tokenizer, dataset, batch_size=4):\n",
        "  model.eval()\n",
        "  num_correct_answers = 0\n",
        "  num_answers = 0\n",
        "  for i in tqdm(range(0, len(dataset), batch_size)):\n",
        "    examples = dataset[i:i+batch_size]\n",
        "    encoding = tokenizer(\n",
        "        examples[\"title\"],\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\",\n",
        "        )\n",
        "    category = torch.tensor(examples[\"category\"])\n",
        "    with torch.no_grad():\n",
        "      outputs = model.forward(**encoding, category=category)\n",
        "    num_correct_answers += (outputs.logits.argmax(-1) == category).sum()\n",
        "    num_answers += len(examples[\"category\"])\n",
        "  model.train()\n",
        "  return num_correct_answers / num_answers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ファインチューニングする前に評価してみる。\n",
        "  * scoreレイヤが未学習なので性能はランダム分類に近い。\n",
        "    * RTX4090なら次のセルは30秒で終わる。"
      ],
      "metadata": {
        "id": "uIVMFKL85Ta7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnk7szEb1kMi"
      },
      "outputs": [],
      "source": [
        "evaluate_by_accuracy(model, tokenizer, dataset[\"validation\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLMのファインチューニング"
      ],
      "metadata": {
        "id": "k4icfTk4IAwF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6nv2dF71kMi"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 評価"
      ],
      "metadata": {
        "id": "GZeWcGlhIDbc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdOlgQhq1kMi"
      },
      "outputs": [],
      "source": [
        "evaluate_by_accuracy(model, tokenizer, dataset[\"validation\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lJTntvR1kMi"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}