{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/course2024-nlp/blob/main/08_sentiment_analysis_with_LLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7UHVLvsl89T"
      },
      "source": [
        "# LLMによる感情分析"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HPNt4EWmD-Y"
      },
      "source": [
        "## 今日の目的\n",
        "* いまどのくらい手軽にLLMを使えるようになっているかを、とりあえず体感する。\n",
        "* 技術的な詳細は次回以降学んでいくことにして、とにかく使ってみる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PtjYmygnqm1"
      },
      "source": [
        "## 今日の内容\n",
        "* 今日は、WRIMEというデータセットを使って、LLMに感情分析させてみる。\n",
        "* 感情分析とは、テキストが表す感情を分析するタスク。\n",
        "* 今回は、ポジティブな感情か、ネガティブな感情かの2値分類タスクとして解く。\n",
        "* LLMとしては`elyza/Llama-3-ELYZA-JP-8B`を使う。\n",
        "  * プロンプトを使ったテキスト生成によって感情分析の問題を解く。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0tqelrXmhun"
      },
      "source": [
        "* **ランタイムのタイプをGPUに設定しておく。**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GIHAP_Rrgkp"
      },
      "source": [
        "## インストール\n",
        "* Hugging Faceの各種ライブラリを使えば、簡単なコードを書くだけでLLMを使える。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-mjfVLBycTx"
      },
      "outputs": [],
      "source": [
        "!pip install -U transformers datasets bitsandbytes accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eX6zotwEoK2r"
      },
      "source": [
        "## 準備"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whM-m4cqzdDt"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9A6Euk7gmXNv"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    set_seed,\n",
        "    BitsAndBytesConfig,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        ")\n",
        "\n",
        "set_seed(123)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELjoQ6q_oTAK"
      },
      "source": [
        "## データセット\n",
        "* WRIME: 主観と客観の感情分析データセット\n",
        "  * https://github.com/ids-cv/wrime\n",
        "* Hugging Faceのdatasets hubに登録されているので、簡単に使える。\n",
        "  * https://huggingface.co/datasets/shunk031/wrime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_CjP_rvdASJ"
      },
      "source": [
        "* `ver2`の方を使う。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4A0Qp1JsoRaE"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\n",
        "    \"shunk031/wrime\",\n",
        "    \"ver2\",\n",
        "    trust_remote_code=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "julWQgfCox2f"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SWy219ERwHg"
      },
      "source": [
        "## 3値分類問題への単純化"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhn-e_k7hqDU"
      },
      "source": [
        "* ラベルから感情を表すテキストへのマッピングをおこなうリストを作っておく。\n",
        "* 元のデータでは、-2, -1, 0, 1, 2の順でネガティブからよりポジティブになる。\n",
        "  * 0がニュートラル。\n",
        "* それぞれの感情をどのような単語で表せば良いかについて、特に正解はない。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lfRDvQ0fTBZ"
      },
      "outputs": [],
      "source": [
        "label_to_text = {\n",
        "    -2: \"ネガティブ\",\n",
        "    -1: \"ネガティブ\",\n",
        "    0: \"ニュートラル\",\n",
        "    1: \"ポジティブ\",\n",
        "    2: \"ポジティブ\",\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACC0ED27qZtV"
      },
      "source": [
        "## LLM\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIsSj4bzq7Cl"
      },
      "source": [
        "* 今回は、elyza/Llama-3-ELYZA-JP-8Bを使う。\n",
        "  * https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B\n",
        "* Google Colab無料版では大きすぎて使えない。\n",
        "* そこで、量子化して使う。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPaMyXHbODyw"
      },
      "source": [
        "* (source:  https://huggingface.co/blog/4bit-transformers-bitsandbytes )\n",
        "![FP8-scheme.png](https://raw.githubusercontent.com/tomonari-masada/course2024-nlp/main/FP8-scheme.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFM4KbnNQwjD"
      },
      "source": [
        "* NF4量子化\n",
        "  * https://huggingface.co/docs/bitsandbytes/en/reference/nn/linear4bit#bitsandbytes.nn.LinearNF4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYR1TtgLrGcv"
      },
      "outputs": [],
      "source": [
        "model_name = \"elyza/Llama-3-ELYZA-JP-8B\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_storage=torch.bfloat16,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    low_cpu_mem_usage=True,\n",
        ").eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfvv_NSCok2s"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UyMWuYsp3uw"
      },
      "source": [
        "### In-context learningを試す"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUANLH-eqAOF"
      },
      "source": [
        "* 対義語を答えさせてみる。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtBg83ARo4eT"
      },
      "outputs": [],
      "source": [
        "text = \"Q:高い\\nA:低い\\n\\nQ:大きい\\nA:小さい\\n\\nQ:狭い\\nA:広い\\n\\nQ:少ない\\nA:多い\\n\\nQ:速い\\nA:遅い\\n\\nQ:嬉しい\\nA:\"\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFj34j7IpWUC"
      },
      "outputs": [],
      "source": [
        "input = tokenizer(text, return_tensors=\"pt\")\n",
        "print(input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sOeyp3cpear"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "  output_ids = model.generate(\n",
        "      **input.to(model.device),\n",
        "      max_new_tokens=10,\n",
        "  )\n",
        "print(output_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ho9b17FFpuJ9"
      },
      "outputs": [],
      "source": [
        "output = tokenizer.decode(\n",
        "    output_ids.tolist()[0][input.input_ids.size(1):], skip_special_tokens=True\n",
        ")\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c33BfNxjypMV"
      },
      "source": [
        "## プロンプト作成用のヘルパ関数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcI9Fsrtco0M"
      },
      "outputs": [],
      "source": [
        "DEFAULT_SYSTEM_PROMPT = \"あなたは誠実で優秀な日本人のアシスタントです。特に指示が無い場合は、常に日本語で回答してください。\\n\\n\"\n",
        "\n",
        "MY_PROMPT = \"次の文章はどのような感情を表していますか。\\nポジティブ、ネガティブ、ニュートラルのいずれかで答えてください。\\n\"\n",
        "\n",
        "def make_input(text):\n",
        "  messages = [\n",
        "      {\"role\": \"system\", \"content\": DEFAULT_SYSTEM_PROMPT},\n",
        "      {\"role\": \"user\", \"content\": MY_PROMPT + text},\n",
        "  ]\n",
        "  prompt = tokenizer.apply_chat_template(\n",
        "      messages,\n",
        "      tokenize=False,\n",
        "      add_generation_prompt=True\n",
        "  )\n",
        "  return tokenizer(prompt, add_special_tokens=False, return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4H03CJDg8Oj"
      },
      "outputs": [],
      "source": [
        "input = make_input(dataset[\"train\"][\"sentence\"][0])\n",
        "input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_yfnNdWhev0E"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.decode(input.input_ids[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PahFnk2z-lU"
      },
      "source": [
        "## 感情分析を行うヘルパ関数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOAAiGFMeSU8"
      },
      "outputs": [],
      "source": [
        "def sentiment_analysis(text):\n",
        "  input = make_input(text)\n",
        "  with torch.no_grad():\n",
        "    output_ids = model.generate(\n",
        "        **input.to(model.device),\n",
        "        max_new_tokens=10,\n",
        "    )\n",
        "  return tokenizer.decode(\n",
        "      output_ids.tolist()[0][input.input_ids.size(1):],\n",
        "      skip_special_tokens=True\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZLY2lMfi9XZ"
      },
      "outputs": [],
      "source": [
        "result = sentiment_analysis(dataset[\"train\"][\"sentence\"][0])\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxgE9qbAfgNg"
      },
      "outputs": [],
      "source": [
        "print(label_to_text[dataset[\"train\"][0][\"avg_readers\"][\"sentiment\"]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRYi_49i1Gdt"
      },
      "source": [
        "### 感情分析の実行"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKPWp6jdW9fF"
      },
      "outputs": [],
      "source": [
        "for _ in range(10):\n",
        "  i = np.random.randint(len(dataset[\"train\"]))\n",
        "  print(f'[{i}]' + '-'*80)\n",
        "  text = dataset[\"train\"][\"sentence\"][i]\n",
        "  print(f\"text:{text}\")\n",
        "  prediction = sentiment_analysis(text)\n",
        "  ground_truth = label_to_text[dataset[\"train\"][i][\"avg_readers\"][\"sentiment\"]]\n",
        "  print(f\"prediction:{prediction}\")\n",
        "  print(f\"ground truth:{ground_truth}\")\n",
        "  print('-'*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cXKf5dF0vRp"
      },
      "source": [
        "# 演習\n",
        "* LLMにもっとうまく感情分析をさせるプロンプトを考えてみよう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUXcB4axyCQO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}