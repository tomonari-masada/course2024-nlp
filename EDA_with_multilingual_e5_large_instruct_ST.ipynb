{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/course2024-nlp/blob/main/EDA_with_multilingual_e5_large_instruct_ST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LtP5EwVUidp"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.functional import normalize\n",
        "\n",
        "from datasets import Dataset, DatasetDict, load_dataset\n",
        "from sentence_transformers import (\n",
        "    SentenceTransformer,\n",
        "    SentenceTransformerTrainer,\n",
        "    SentenceTransformerTrainingArguments,\n",
        ")\n",
        "from sentence_transformers.losses import TripletLoss\n",
        "from sentence_transformers.evaluation import TripletEvaluator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbsbkCo8Uidq"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\n",
        "    \"shunk031/livedoor-news-corpus\",\n",
        "    train_ratio=0.8, val_ratio=0.1, test_ratio=0.1,\n",
        "    random_state=42,\n",
        "    shuffle=True,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "num_categories = len(set(dataset[\"train\"][\"category\"]))\n",
        "max_seq_length = 512\n",
        "\n",
        "category_names = ['movie-enter', 'it-life-hack', 'kaden-channel', 'topic-news', 'livedoor-homme', 'peachy', 'sports-watch', 'dokujo-tsushin', 'smax']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHKbPWfzUidq"
      },
      "outputs": [],
      "source": [
        "dataset[\"train\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lO8FJFW9Uidr"
      },
      "outputs": [],
      "source": [
        "np.random.seed(1234)\n",
        "\n",
        "triplet_dataset = {}\n",
        "for key in dataset:\n",
        "\n",
        "    categorized = [list() for i in range(num_categories)]\n",
        "    for example in dataset[key]:\n",
        "        categorized[example[\"category\"]].append(example[\"title\"])\n",
        "    category_size = [len(categorized[i]) for i in range(num_categories)]\n",
        "\n",
        "    anchors, positives, negatives = [], [], []\n",
        "    for i in range(num_categories):\n",
        "        indices = i + np.random.randint(1, num_categories, category_size[i])\n",
        "        indices = indices % num_categories\n",
        "        anchors += categorized[i]\n",
        "        positives += [\n",
        "            categorized[i][np.random.randint(0, category_size[i])]\n",
        "            for _ in indices\n",
        "        ]\n",
        "        negatives += [\n",
        "            categorized[j][np.random.randint(0, category_size[j])]\n",
        "            for j in indices\n",
        "        ]\n",
        "\n",
        "    triplet_dataset[key] = Dataset.from_dict({\n",
        "        \"anchors\": anchors,\n",
        "        \"positives\": positives,\n",
        "        \"negatives\": negatives,\n",
        "    })\n",
        "\n",
        "triplet_dataset = DatasetDict(triplet_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZZD3tgKUidr"
      },
      "outputs": [],
      "source": [
        "train_dataset = triplet_dataset[\"train\"]\n",
        "eval_dataset = triplet_dataset[\"validation\"]\n",
        "test_dataset = triplet_dataset[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLCE-XSzUidr"
      },
      "outputs": [],
      "source": [
        "train_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mswgAAvgUidr"
      },
      "outputs": [],
      "source": [
        "model_id = \"intfloat/multilingual-e5-large-instruct\"\n",
        "model = SentenceTransformer(model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgF3PttJUidr"
      },
      "outputs": [],
      "source": [
        "loss = TripletLoss(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9aNYqGxUidr"
      },
      "outputs": [],
      "source": [
        "args = SentenceTransformerTrainingArguments(\n",
        "    output_dir=f\"models/{model_id}_livedoor-title-triplet\",\n",
        "    max_steps=1000,\n",
        "    learning_rate=1e-4,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=8,\n",
        "    bf16=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=100,\n",
        "    save_total_limit=2,\n",
        "    logging_steps=100,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-cAXd9mUids"
      },
      "outputs": [],
      "source": [
        "dev_evaluator = TripletEvaluator(\n",
        "    anchors=eval_dataset[\"anchors\"],\n",
        "    positives=eval_dataset[\"positives\"],\n",
        "    negatives=eval_dataset[\"negatives\"],\n",
        ")\n",
        "dev_evaluator(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItVSVgaZUids"
      },
      "outputs": [],
      "source": [
        "trainer = SentenceTransformerTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    loss=loss,\n",
        "    evaluator=dev_evaluator,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TpeROacUids"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KdIeApb8Uids"
      },
      "outputs": [],
      "source": [
        "def print_trainable_parameters(model):\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} \"\n",
        "        f\"|| all params: {all_param} \"\n",
        "        f\"|| trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSfAGxlfUids"
      },
      "outputs": [],
      "source": [
        "print_trainable_parameters(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-jri-_RUids"
      },
      "outputs": [],
      "source": [
        "for key in dataset:\n",
        "  print(key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1G5W2SDBUids"
      },
      "outputs": [],
      "source": [
        "len(dataset[\"train\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-YO6l-GUidt"
      },
      "outputs": [],
      "source": [
        "embeddings = {}\n",
        "for key in dataset:\n",
        "    embeddings[key] = model.encode(\n",
        "        dataset[key][\"title\"],\n",
        "        normalize_embeddings=True,\n",
        "        show_progress_bar=True,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THp84hHTUidt"
      },
      "outputs": [],
      "source": [
        "embeddings[\"train\"].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxaFTGfSUidt"
      },
      "outputs": [],
      "source": [
        "n_clusters = 30\n",
        "kmeans = KMeans(n_clusters=n_clusters, n_init='auto', random_state=123)\n",
        "kmeans.fit(embeddings[\"train\"])\n",
        "centers = kmeans.cluster_centers_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6gp0cEOUidt"
      },
      "outputs": [],
      "source": [
        "unique, counts = np.unique(kmeans.labels_, return_counts=True)\n",
        "size_dict = dict(zip(unique, counts))\n",
        "print(sorted([item[1] for item in size_dict.items()]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIZaO4CRUidt"
      },
      "outputs": [],
      "source": [
        "label_pos_tags = [\"NOUN\", \"VERB\", \"PROPN\"]\n",
        "\n",
        "nlp = spacy.load(\"ja_core_news_sm\")\n",
        "corpus = {}\n",
        "for key in dataset:\n",
        "    corpus[key] = []\n",
        "    for text in tqdm(dataset[key][\"title\"]):\n",
        "        corpus[key].append(\" \".join(\n",
        "            [token.lemma_\n",
        "             for token in nlp(text) if token.pos_ in label_pos_tags\n",
        "            ]\n",
        "        ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nBQR04XUidt"
      },
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer(min_df=10, max_df=0.1, lowercase=False)\n",
        "vectorizer.fit(corpus[\"train\"])\n",
        "vocab = np.array(vectorizer.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxoJMshbUidt"
      },
      "outputs": [],
      "source": [
        "vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qT16bTXlUidt"
      },
      "outputs": [],
      "source": [
        "vocab_embeddings = model.encode(list(vocab), normalize_embeddings=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KpJcPi6xUidt"
      },
      "outputs": [],
      "source": [
        "topic_words = []\n",
        "similarities = cosine_similarity(vocab_embeddings, centers)\n",
        "for i in range(similarities.shape[-1]):\n",
        "    indices = np.argsort(- similarities[:,i])\n",
        "    topic_words.append(f\"{i:d} \" + \" \".join(list(vocab[indices[:20]])))\n",
        "print(\"\\n\".join(topic_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6529ipMMUidu"
      },
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer(min_df=10, max_df=0.1, lowercase=False)\n",
        "vectorizer.fit(corpus[\"train\"])\n",
        "vocab = np.array(vectorizer.get_feature_names_out())\n",
        "X_train = vectorizer.transform(corpus[\"train\"]).toarray()\n",
        "\n",
        "vocab_embeddings = np.dot((X_train / X_train.sum(0)).T, embeddings[\"train\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfZRDAfGUidu"
      },
      "outputs": [],
      "source": [
        "topic_words = []\n",
        "similarities = cosine_similarity(vocab_embeddings, centers)\n",
        "for i in range(similarities.shape[-1]):\n",
        "    indices = np.argsort(- similarities[:,i])\n",
        "    topic_words.append(f\"{i:d} \" + \" \".join(list(vocab[indices[:20]])))\n",
        "print(\"\\n\".join(topic_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G98FcKmrUidu"
      },
      "outputs": [],
      "source": [
        "for param in model.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKHQUqPeUidu"
      },
      "outputs": [],
      "source": [
        "model.to(\"cpu\").eval();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7R4Ln9TzUidu"
      },
      "outputs": [],
      "source": [
        "from captum.attr import LayerIntegratedGradients, TokenReferenceBase, visualization\n",
        "\n",
        "tokenizer = model.tokenizer\n",
        "token_reference = TokenReferenceBase(reference_token_idx=tokenizer.pad_token_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbIXF1yDUidu"
      },
      "outputs": [],
      "source": [
        "text = dataset[\"train\"][\"title\"][0]\n",
        "encodings = tokenizer(text, padding=True, return_tensors=\"pt\")\n",
        "encodings = encodings.to(model.device)\n",
        "input_ids = encodings[\"input_ids\"]\n",
        "attention_mask = encodings[\"attention_mask\"]\n",
        "with torch.no_grad():\n",
        "    embedding = model({\"input_ids\": input_ids, \"attention_mask\": attention_mask})[\"sentence_embedding\"]\n",
        "normalize(embedding, p=2, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FuI7-JKyUidu"
      },
      "outputs": [],
      "source": [
        "cos_sim = nn.CosineSimilarity(dim=-1)\n",
        "cluster_centers = torch.tensor(kmeans.cluster_centers_, device=model.device)\n",
        "\n",
        "def predict(input_ids, attention_mask):\n",
        "    embedding = model({\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "    })[\"sentence_embedding\"]\n",
        "    embedding = normalize(embedding, p=2, dim=1)\n",
        "    return cos_sim(\n",
        "        cluster_centers.unsqueeze(0),\n",
        "        embedding.unsqueeze(1)\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rQvRyL9Uidu"
      },
      "outputs": [],
      "source": [
        "text = dataset[\"train\"][\"title\"][0]\n",
        "encodings = tokenizer(text, padding=True, return_tensors=\"pt\")\n",
        "encodings.to(model.device)\n",
        "predict(\n",
        "    encodings.input_ids,\n",
        "    encodings.attention_mask,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYSgFSEnUidu"
      },
      "outputs": [],
      "source": [
        "def cluster_similarity_forward_func(input_ids, attention_mask, cluster_id):\n",
        "    similarities = predict(input_ids, attention_mask)\n",
        "    return similarities[:,cluster_id]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pov6TQv0Uidv"
      },
      "outputs": [],
      "source": [
        "text = dataset[\"train\"][\"title\"][0]\n",
        "encodings = tokenizer(text, padding=True, return_tensors=\"pt\")\n",
        "encodings.to(model.device)\n",
        "cluster_similarity_forward_func(\n",
        "    encodings.input_ids,\n",
        "    encodings.attention_mask,\n",
        "    29,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9BJ246gUidv"
      },
      "outputs": [],
      "source": [
        "list(model[0].modules())[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ya_q8wDCUidv"
      },
      "outputs": [],
      "source": [
        "list(model[0].modules())[1].embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJzKbW0FUidv"
      },
      "outputs": [],
      "source": [
        "lig = LayerIntegratedGradients(\n",
        "    cluster_similarity_forward_func,\n",
        "    list(model[0].modules())[1].embeddings.word_embeddings,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H30SfWGYUidv"
      },
      "outputs": [],
      "source": [
        "vis_data_records_ig = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8S8w7qfuUidv"
      },
      "outputs": [],
      "source": [
        "def add_attributions_to_visualizer(attributions, text, pred_prob, pred_class, true_class,\n",
        "                                   attr_class, convergence_scores, vis_data_records):\n",
        "    attributions = attributions.cpu()\n",
        "    attributions = attributions.sum(dim=-1).squeeze(0)\n",
        "    attributions = attributions / torch.norm(attributions)\n",
        "    attributions = attributions.cpu().detach().numpy()\n",
        "    vis_data_records.append(\n",
        "        visualization.VisualizationDataRecord(\n",
        "            attributions,\n",
        "            pred_prob,\n",
        "            pred_class,\n",
        "            true_class,\n",
        "            attr_class,\n",
        "            attributions.sum(),\n",
        "            text,\n",
        "            convergence_scores,\n",
        "        )\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6P3mf-VxUidv"
      },
      "outputs": [],
      "source": [
        "def interpret_text(text, attr_class=None, n_steps=50):\n",
        "    encodings = tokenizer(text, padding=True, return_tensors=\"pt\")\n",
        "    encodings = encodings.to(model.device)\n",
        "    input_ids = encodings.input_ids\n",
        "    attention_mask = encodings.attention_mask\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
        "    reference_input_ids = token_reference.generate_reference(\n",
        "        len(tokens),\n",
        "        device=model.device,\n",
        "    ).unsqueeze(0)\n",
        "\n",
        "    similarities = predict(\n",
        "        input_ids,\n",
        "        attention_mask,\n",
        "    )\n",
        "    prediction = similarities.argmax().item()\n",
        "    if attr_class is None:\n",
        "        attr_class = prediction\n",
        "    print(\n",
        "        f\"prediction={prediction} \"\n",
        "        f\"cos_sim={similarities.max().item():.3f} \",\n",
        "        end=\"\"\n",
        "    )\n",
        "\n",
        "    attributions_ig, delta = lig.attribute(\n",
        "        input_ids,\n",
        "        reference_input_ids,\n",
        "        additional_forward_args=(attention_mask, attr_class),\n",
        "        n_steps=n_steps,\n",
        "        return_convergence_delta=True,\n",
        "    )\n",
        "    print(f\"convergence delta={delta.item():.3e} when n_steps={n_steps}\")\n",
        "\n",
        "    add_attributions_to_visualizer(\n",
        "        attributions_ig,\n",
        "        tokens,\n",
        "        similarities.max().item(),\n",
        "        str(prediction),\n",
        "        str(prediction),\n",
        "        str(attr_class),\n",
        "        delta,\n",
        "        vis_data_records_ig,\n",
        "    )\n",
        "    return prediction\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vw8hJGlAUidv"
      },
      "outputs": [],
      "source": [
        "vis_data_records_ig = []\n",
        "for n_steps in [50, 100, 200, 300]:\n",
        "    interpret_text(dataset[\"train\"][\"title\"][0], n_steps=n_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVPh8cuQUidw"
      },
      "outputs": [],
      "source": [
        "visualization.visualize_text(vis_data_records_ig);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApanHAvcUidw"
      },
      "outputs": [],
      "source": [
        "for i in tqdm(range(50)):\n",
        "    example = dataset[\"validation\"][i]\n",
        "    print(category_names[example[\"category\"]], end=\" \")\n",
        "    vis_data_records_ig = []\n",
        "    prediction = interpret_text(example[\"title\"], n_steps=50)\n",
        "    print(\"\\t\" + topic_words[prediction])\n",
        "    visualization.visualize_text(vis_data_records_ig);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Hcp0xozUidw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}