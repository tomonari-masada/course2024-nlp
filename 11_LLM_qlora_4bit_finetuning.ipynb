{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/course2024-nlp/blob/main/11_LLM_qlora_4bit_finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COO6E0ZupoJ9"
      },
      "source": [
        "# LLMのファインチューニング: テキスト生成による分類"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVBbQHDppoJ_"
      },
      "source": [
        "* `kunishou/databricks-dolly-15k-ja`というデータセットを使う\n",
        "  * https://huggingface.co/datasets/kunishou/databricks-dolly-15k-ja\n",
        "  * `category`が`classification`のrowだけを使う。\n",
        "  * 分類問題とはいえ、テキストで答えるようになっている。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SE6sfmhYpoKA"
      },
      "source": [
        "## インストール"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUjRtuVppoKA"
      },
      "outputs": [],
      "source": [
        "!pip install -U transformers trl peft datasets accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaZQAlpWpoKB"
      },
      "source": [
        "## 再現性の確保"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFqj36DopoKB"
      },
      "outputs": [],
      "source": [
        "from transformers import set_seed\n",
        "\n",
        "set_seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-hOdWYwpoKC"
      },
      "source": [
        "## 使用するLLMの指定"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gemV2GBWpoKC"
      },
      "outputs": [],
      "source": [
        "model_id = \"rinna/gemma-2-baku-2b-it\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqRxDqUepoKC"
      },
      "source": [
        "## データセット"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wA0qnH4poKC"
      },
      "source": [
        "### データセットの取得"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AftxI46TpoKD"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "dataset = load_dataset(\"kunishou/databricks-dolly-15k-ja\")\n",
        "dataset = dataset.filter(lambda example: example[\"category\"] == \"classification\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7_7bhcQpoKD"
      },
      "source": [
        "### データセットをtraining/validation/test setsに分割する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56zLBNHjpoKD"
      },
      "outputs": [],
      "source": [
        "train_test_ds = dataset[\"train\"].train_test_split(test_size=0.1, seed=1234)\n",
        "valid_test_ds = train_test_ds[\"test\"].train_test_split(test_size=0.5, seed=1234)\n",
        "\n",
        "train_ds = train_test_ds[\"train\"]\n",
        "valid_ds = valid_test_ds[\"train\"]\n",
        "test_ds = valid_test_ds[\"test\"]\n",
        "\n",
        "print(len(train_ds), len(valid_ds), len(test_ds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4X8_TuGJpoKD"
      },
      "outputs": [],
      "source": [
        "train_ds[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWP82eRLpoKD"
      },
      "source": [
        "## 学習のためのプロンプト"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kw4jbJwOpoKD"
      },
      "source": [
        "* 答えも含んだプロンプトを作る。\n",
        "  * これを使って通常の言語モデルとしての学習をおこなう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3P5iHEhpoKD"
      },
      "outputs": [],
      "source": [
        "def make_training_prompt(example):\n",
        "  message = [\n",
        "    {\n",
        "      'role': 'user',\n",
        "      'content': example['instruction']\n",
        "    },\n",
        "    {\n",
        "      'role': 'model',\n",
        "      'content': example['output']\n",
        "    }\n",
        "  ]\n",
        "  return tokenizer.apply_chat_template(message, tokenize=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tzN1Q23poKD"
      },
      "outputs": [],
      "source": [
        "make_training_prompt(train_ds[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7J_tXUKCpoKE"
      },
      "source": [
        "### training setの前処理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9ytTNUtpoKE"
      },
      "outputs": [],
      "source": [
        "def add_text(example):\n",
        "  example[\"text\"] = make_training_prompt(example)\n",
        "  return example\n",
        "\n",
        "train_ds = train_ds.map(add_text)\n",
        "train_ds = train_ds.remove_columns([\"input\", \"category\", \"output\", \"index\", \"instruction\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DbZe7k4tpoKE"
      },
      "outputs": [],
      "source": [
        "train_ds[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sG9OzrEOpoKE"
      },
      "source": [
        "### validation setの前処理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SZ0nOWApoKE"
      },
      "outputs": [],
      "source": [
        "valid_ds = valid_ds.map(add_text)\n",
        "valid_ds = valid_ds.remove_columns([\"input\", \"category\", \"output\", \"index\", \"instruction\"])\n",
        "valid_ds[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdL3pLxJpoKE"
      },
      "source": [
        "## 評価のためのプロンプト"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqh45wffpoKE"
      },
      "source": [
        "* 評価の際はLLMに答えを作らせるので、答えを含まないプロンプトを作る。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8D-_wIwRpoKE"
      },
      "outputs": [],
      "source": [
        "def make_eval_prompt(example):\n",
        "  message = [\n",
        "    {\n",
        "      'role': 'user',\n",
        "      'content': example['instruction']\n",
        "    }\n",
        "  ]\n",
        "  return tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQyMX8XipoKE"
      },
      "outputs": [],
      "source": [
        "make_eval_prompt(test_ds[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UN95qBi7poKE"
      },
      "source": [
        "### モデルに答えさせてみる"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZAUy-42poKE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "dtype = torch.bfloat16\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "  model_id,\n",
        "  torch_dtype=dtype,\n",
        ").to(\"cuda:0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Muwf7abRpoKE"
      },
      "outputs": [],
      "source": [
        "prompt = make_eval_prompt(test_ds[0])\n",
        "inputs = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
        "outputs = model.generate(input_ids=inputs.to(model.device), max_new_tokens=300)\n",
        "print(tokenizer.decode(outputs[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2Xb2ld9poKF"
      },
      "outputs": [],
      "source": [
        "test_ds[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ここでモデルの中身を見ておく。"
      ],
      "metadata": {
        "id": "BBZCB_13rxql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "u_bI8vo6rwq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* モデルをいったん削除する。\n",
        "  * GPUのメモリも解放する。"
      ],
      "metadata": {
        "id": "y8NN2bN7tads"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HL6ykByupoKF"
      },
      "outputs": [],
      "source": [
        "del model\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEm84eCspoKF"
      },
      "source": [
        "## 量子化＋LoRAによるfinetuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xn7TvyTjpoKF"
      },
      "source": [
        "### LoRAの設定"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqjoVayjpoKF"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, TaskType\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "  task_type=TaskType.CAUSAL_LM,\n",
        "  inference_mode=False,\n",
        "  r=8,\n",
        "  lora_alpha=32,\n",
        "  lora_dropout=0.1,\n",
        "  target_modules=[\n",
        "    \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "    #\"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "  ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWTYQM1ipoKF"
      },
      "source": [
        "### 量子化の設定"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sp5iLa-kpoKF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "  load_in_4bit=True,\n",
        "  bnb_4bit_use_double_quant=True,\n",
        "  bnb_4bit_quant_type=\"nf4\",\n",
        "  bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### モデルの読み込み"
      ],
      "metadata": {
        "id": "ARMKjitXtUqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "  model_id,\n",
        "  torch_dtype=dtype,\n",
        "  low_cpu_mem_usage=True,\n",
        "  quantization_config=bnb_config,\n",
        ")"
      ],
      "metadata": {
        "id": "eUYrvKSHtQk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HceaY77qpoKF"
      },
      "source": [
        "### 学習の設定"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VMTWeIjpoKJ"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "  output_dir=\"./results\",\n",
        "  num_train_epochs=1,\n",
        "  learning_rate=2e-4,\n",
        "  max_steps=500,\n",
        "  logging_steps=100,\n",
        "  per_device_train_batch_size=1,\n",
        "  per_device_eval_batch_size=1,\n",
        "  gradient_accumulation_steps=4,\n",
        "  optim = 'adamw_torch',\n",
        "  eval_strategy=\"steps\",\n",
        "  eval_steps=100,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFb6vLsQpoKJ"
      },
      "source": [
        "### `SFTTrainer`の作成"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rYfVpAUpoKK"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "  model=model,\n",
        "  peft_config=peft_config,\n",
        "  train_dataset=train_ds,\n",
        "  eval_dataset=valid_ds,\n",
        "  dataset_text_field=\"text\",\n",
        "  args=training_args,\n",
        "  max_seq_length=512, #512にしないとGoogle Colab無料版で走らない\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EyFwKrmpoKK"
      },
      "outputs": [],
      "source": [
        "def print_trainable_parameters(model, verbose=False):\n",
        "  trainable_params = 0\n",
        "  all_param = 0\n",
        "  for name, param in model.named_parameters():\n",
        "    all_param += param.numel()\n",
        "    if param.requires_grad:\n",
        "      trainable_params += param.numel()\n",
        "      if verbose: print(name)\n",
        "  print(\n",
        "      f\"trainable params: {trainable_params} \"\n",
        "      f\"|| all params: {all_param} \"\n",
        "      f\"|| trainable%: {100 * trainable_params / all_param}\"\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7RfjaoQBpoKK"
      },
      "outputs": [],
      "source": [
        "print_trainable_parameters(trainer.model, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hINfrGhMpoKK"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWvjXHUrpoKK"
      },
      "outputs": [],
      "source": [
        "lora_adaptor = \"lora/\" + model_id + \"-QLoRA-4bit-double\"\n",
        "trainer.save_model(lora_adaptor)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del model\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "bDImLFpK0Krm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aifS9LNGpoKK"
      },
      "outputs": [],
      "source": [
        "from peft import PeftModel\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "  model_id,\n",
        "  device_map=\"auto\",\n",
        "  quantization_config=bnb_config,\n",
        ")\n",
        "\n",
        "model = PeftModel.from_pretrained(model, lora_adaptor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oA9b6PhSpoKK"
      },
      "outputs": [],
      "source": [
        "prompt = make_eval_prompt(test_ds[0])\n",
        "inputs = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
        "outputs = model.generate(input_ids=inputs.to(model.device), max_new_tokens=300)\n",
        "print(tokenizer.decode(outputs[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jc6LJl2_poKK"
      },
      "source": [
        "## 評価の仕方\n",
        "* langchainのQAEvalChainを使う。\n",
        "  * 説明は省略します。私自身は使ったことがないですので・・・。\n",
        "  * 評価用のLLMとしては、OpenAIの`gpt-3.5-turbo-instruct`あたりに課金すると安いかも。"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cetD36JH3X5v"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}